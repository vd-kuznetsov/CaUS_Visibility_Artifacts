{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "2Dv7VN6WSSTT",
        "LsyGsaEV-f4F"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download repository and install libraries "
      ],
      "metadata": {
        "id": "YN9R2eK6Nh3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CVI-SZU/CCAM\n",
        "%cd CCAM\n",
        "!git reset --hard 9902101f7731f37cb85e3f6b1ddeeae274603ab1\n",
        "%cd .."
      ],
      "metadata": {
        "id": "CPWicDZsF_54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYTxde6IIoWD"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX\n",
        "!pip install cmapy\n",
        "!pip install imgaug -U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare"
      ],
      "metadata": {
        "id": "RomlgsyB5JkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model weights"
      ],
      "metadata": {
        "id": "u-H00I00mJab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 18M2C19a21AWlaLOQuj8o3QYKOL1hSR8_\n",
        "!gdown 1uBfgxkAvE1eqBsZzygyneSj2bL-U961x"
      ],
      "metadata": {
        "id": "Z779_qgnHjK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Work with Data"
      ],
      "metadata": {
        "id": "pTngpDHo3HQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "# dataset part 1\n",
        "url = \"https://drive.google.com/drive/folders/1AxqLHFCjtoDiJjD0lweiOzVuTDZgla3h\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)\n",
        "\n",
        "# dataset part 2\n",
        "url = \"https://drive.google.com/drive/folders/15z3pimTCFH9ywO9oNgpGC6Ff_04Z4O_z\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)\n",
        "\n",
        "!mv Dataset_Part_1/ dataset/\n",
        "!mv Dataset_Part_2/ dataset1/"
      ],
      "metadata": {
        "id": "iOmYetFk45fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset/ACDC_Test_Drops.zip -d /content/dataset/\n",
        "!rm dataset/ACDC_Test_Drops.zip\n",
        "!unzip dataset/ACDC_Train_Drops.zip -d /content/dataset/\n",
        "!rm dataset/ACDC_Train_Drops.zip\n",
        "!unzip dataset/ACDC_Val_Drops.zip -d /content/dataset/\n",
        "!rm dataset/ACDC_Val_Drops.zip\n",
        "!unzip dataset/clean_acdc.zip -d /content/dataset/clean_acdc\n",
        "!rm dataset/clean_acdc.zip\n",
        "!unzip dataset/clean_test_acdc.zip -d /content/dataset/clean_test_acdc\n",
        "!rm dataset/clean_test_acdc.zip\n",
        "!unzip dataset/clean_test_woodscape.zip -d /content/dataset/clean_test_woodscape\n",
        "!rm dataset/clean_test_woodscape.zip\n",
        "!unzip dataset/defocus_test_woodscape.zip -d /content/dataset/defocus_test_woodscape\n",
        "!rm dataset/defocus_test_woodscape.zip\n",
        "!unzip dataset/drops_test_woodscape.zip -d /content/dataset/drops_test_woodscape\n",
        "!rm dataset/drops_test_woodscape.zip\n",
        "!unzip dataset/full_loss_test_woodscape.zip -d /content/dataset/full_loss_test_woodscape\n",
        "!rm dataset/full_loss_test_woodscape.zip\n",
        "!unzip dataset/motion_test_acdc.zip -d /content/dataset/motion_test_acdc\n",
        "!rm dataset/motion_test_acdc.zip\n",
        "!unzip dataset/motion_test_woodscape.zip -d /content/dataset/motion_test_woodscape\n",
        "!rm dataset/motion_test_woodscape.zip\n",
        "!unzip dataset/partial_test_woodscape.zip -d /content/dataset/partial_test_woodscape\n",
        "!rm dataset/partial_test_woodscape.zip\n",
        "!unzip dataset/SoilingDatasetNotFishEye_Test.zip -d /content/dataset/\n",
        "!rm dataset/SoilingDatasetNotFishEye_Test.zip\n",
        "!unzip dataset/SoilingDatasetNotFishEye_Train.zip -d /content/dataset/\n",
        "!rm dataset/SoilingDatasetNotFishEye_Train.zip\n",
        "!unzip dataset/spatter_acdc.zip -d /content/dataset/spatter_acdc\n",
        "!rm dataset/spatter_acdc.zip\n",
        "!unzip dataset/spatter_test_acdc.zip -d /content/dataset/spatter_test_acdc\n",
        "!rm dataset/spatter_test_acdc.zip\n",
        "!unzip dataset/Tapmer_Test_FullLoss.zip -d /content/dataset/\n",
        "!rm dataset/Tapmer_Test_FullLoss.zip\n",
        "!unzip dataset/Tapmer_Train_FullLoss.zip -d /content/dataset/\n",
        "!rm dataset/Tapmer_Train_FullLoss.zip"
      ],
      "metadata": {
        "id": "e85YgClQ9BrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset1/clean_driving.zip -d /content/dataset1/clean_driving\n",
        "!rm dataset1/clean_driving.zip\n",
        "!unzip dataset1/clean_woodscape.zip -d /content/dataset1/clean_woodscape\n",
        "!rm dataset1/clean_woodscape.zip\n",
        "!unzip dataset1/defocus_acdc.zip -d /content/dataset1/defocus_acdc\n",
        "!rm dataset1/defocus_acdc.zip\n",
        "!unzip dataset1/defocus_driving.zip -d /content/dataset1/defocus_driving\n",
        "!rm dataset1/defocus_driving.zip\n",
        "!unzip dataset1/defocus_woodscape.zip -d /content/dataset1/defocus_woodscape\n",
        "!rm dataset1/defocus_woodscape.zip\n",
        "!unzip dataset1/defocus_test_acdc.zip -d /content/dataset1/defocus_test_acdc\n",
        "!rm dataset1/defocus_test_acdc.zip\n",
        "!unzip dataset1/drops_woodscape.zip -d /content/dataset1/drops_woodscape\n",
        "!rm dataset1/drops_woodscape.zip\n",
        "!unzip dataset1/full_loss_acdc_1000.zip -d /content/dataset1/full_loss_acdc_1000\n",
        "!rm dataset1/full_loss_acdc_1000.zip\n",
        "!unzip dataset1/full_loss_woodscape.zip -d /content/dataset1/full_loss_woodscape\n",
        "!rm dataset1/full_loss_woodscape.zip\n",
        "!unzip dataset1/motion_acdc.zip -d /content/dataset1/motion_acdc\n",
        "!rm dataset1/motion_acdc.zip\n",
        "!unzip dataset1/motion_driving.zip -d /content/dataset1/motion_driving\n",
        "!rm dataset1/motion_driving.zip\n",
        "!unzip dataset1/motion_woodscape.zip -d /content/dataset1/motion_woodscape\n",
        "!rm dataset1/motion_woodscape.zip\n",
        "!unzip dataset1/partial_woodscape.zip -d /content/dataset1/partial_woodscape\n",
        "!rm dataset1/partial_woodscape.zip"
      ],
      "metadata": {
        "id": "aCZ73d4C9F1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv dataset/SoilingDatasetNotFishEye_Test /content/\n",
        "!mv dataset/SoilingDatasetNotFishEye_Train /content/\n",
        "\n",
        "!mv SoilingDatasetNotFishEye_Test test\n",
        "!mv SoilingDatasetNotFishEye_Train train\n",
        "\n",
        "!bash -c 'mv dataset/Tapmer_Train_FullLoss/*.* /content/train/full'\n",
        "!bash -c 'mv dataset/Tapmer_Test_FullLoss/*.* /content/test/full'"
      ],
      "metadata": {
        "id": "otmv-wB79IW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def data_transfer(old_folder, new_folder, break_number):\n",
        "    root = \"\"\n",
        "    for root, dirs, files in os.walk(os.path.join(root, old_folder)):\n",
        "        dirs.sort()\n",
        "        i = 0\n",
        "        for file in files:\n",
        "            image_name = os.path.join(root, file)\n",
        "            try: os.replace(image_name, new_folder + image_name.split('/')[3])\n",
        "            except: os.replace(image_name, new_folder + image_name.split('/')[2])\n",
        "            i = i + 1\n",
        "            if i == break_number: break\n",
        "        print(i)\n",
        "        break"
      ],
      "metadata": {
        "id": "qp-dDC0v9LVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transfer(\"dataset1/full_loss_acdc_1000/full_loss\", \"test/full/\", 93)\n",
        "data_transfer(\"dataset1/full_loss_woodscape/full_loss\", \"test/full/\", 92)\n",
        "data_transfer(\"dataset/full_loss_test_woodscape/full_loss_test\", \"test/full/\", 92)\n",
        "\n",
        "data_transfer(\"dataset1/full_loss_acdc_1000/full_loss\", \"train/full/\", -1)\n",
        "data_transfer(\"dataset1/full_loss_woodscape/full_loss\", \"train/full/\", -1)\n",
        "data_transfer(\"dataset/full_loss_test_woodscape/full_loss_test\", \"train/full/\", -1)\n",
        "\n",
        "!mkdir train/defocus\n",
        "!mkdir test/defocus\n",
        "\n",
        "data_transfer(\"dataset1/defocus_acdc/defocus\", \"test/defocus/\", 205)\n",
        "data_transfer(\"dataset/defocus_test_woodscape/defocus_test\", \"test/defocus/\", 205)\n",
        "data_transfer(\"dataset1/defocus_driving/defocus_driving\", \"test/defocus/\", 87)\n",
        "data_transfer(\"dataset1/defocus_test_acdc/defocus\", \"test/defocus/\", 102)\n",
        "\n",
        "data_transfer(\"dataset1/defocus_acdc/defocus\", \"train/defocus/\", -1)\n",
        "data_transfer(\"dataset/defocus_test_woodscape/defocus_test\", \"train/defocus/\", -1)\n",
        "data_transfer(\"dataset1/defocus_driving/defocus_driving\", \"train/defocus/\", -1)\n",
        "data_transfer(\"dataset1/defocus_test_acdc/defocus\", \"train/defocus/\", -1)\n",
        "data_transfer(\"dataset1/defocus_woodscape/defocus\", \"train/defocus/\", -1)\n",
        "\n",
        "!mkdir train/motion\n",
        "!mkdir test/motion\n",
        "\n",
        "data_transfer(\"dataset1/motion_acdc/motion\", \"test/motion/\", 205)\n",
        "data_transfer(\"dataset/motion_test_woodscape/motion_test\", \"test/motion/\", 205)\n",
        "data_transfer(\"dataset1/motion_driving/motion_driving\", \"test/motion/\", 87)\n",
        "data_transfer(\"dataset/motion_test_acdc/motion\", \"test/motion/\", 102)\n",
        "\n",
        "data_transfer(\"dataset1/motion_acdc/motion\", \"train/motion/\", -1)\n",
        "data_transfer(\"dataset/motion_test_woodscape/motion_test\", \"train/motion/\", -1)\n",
        "data_transfer(\"dataset1/motion_driving/motion_driving\", \"train/motion/\", -1)\n",
        "data_transfer(\"dataset/motion_test_acdc/motion\", \"train/motion/\", -1)\n",
        "data_transfer(\"dataset1/motion_woodscape/motion\", \"train/motion/\", -1)\n",
        "\n",
        "data_transfer(\"dataset/spatter_test_acdc/spatter\", \"test/partial/\", 225)\n",
        "data_transfer(\"dataset/partial_test_woodscape/partial_test\", \"test/partial/\", 210)\n",
        "\n",
        "data_transfer(\"dataset/spatter_acdc/spatter\", \"train/partial/\", -1)\n",
        "data_transfer(\"dataset/spatter_test_acdc/spatter\", \"train/partial/\", -1)\n",
        "data_transfer(\"dataset/partial_test_woodscape/partial_test\", \"train/partial/\", -1)\n",
        "data_transfer(\"dataset1/partial_woodscape/partial\", \"train/partial/\", -1)\n",
        "\n",
        "!mv dataset/ACDC_Val_Drops/*.* test/drops\n",
        "\n",
        "data_transfer(\"dataset/ACDC_Train_Drops\", \"test/drops/\", 181)\n",
        "data_transfer(\"dataset/drops_test_woodscape/drops_test\", \"test/drops/\", 181)\n",
        "\n",
        "!mv dataset/ACDC_Test_Drops/*.* train/drops\n",
        "\n",
        "data_transfer(\"dataset/ACDC_Train_Drops\", \"train/drops/\", -1)\n",
        "data_transfer(\"dataset/drops_test_woodscape/drops_test\", \"train/drops/\", -1)\n",
        "data_transfer(\"dataset1/drops_woodscape/drops\", \"train/drops/\", -1)\n",
        "\n",
        "data_transfer(\"dataset/clean_test_acdc/clean\", \"test/clean/\", 202)\n",
        "data_transfer(\"dataset/clean_test_woodscape/clean_test\", \"test/clean/\", 202)\n",
        "data_transfer(\"dataset1/clean_driving/clean_driving\", \"test/clean/\", 202)\n",
        "\n",
        "data_transfer(\"dataset/clean_test_acdc/clean\", \"train/clean/\", -1)\n",
        "data_transfer(\"dataset/clean_acdc/clean\", \"train/clean/\", -1)\n",
        "data_transfer(\"dataset/clean_test_woodscape/clean_test\", \"train/clean/\", -1)\n",
        "data_transfer(\"dataset1/clean_driving/clean_driving\", \"train/clean/\", -1)\n",
        "data_transfer(\"dataset1/clean_woodscape/clean\", \"train/clean/\", -1)"
      ],
      "metadata": {
        "id": "NXFsTy3_9NpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train CCAM"
      ],
      "metadata": {
        "id": "UO8rrn4H5q9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## datasets.py"
      ],
      "metadata": {
        "id": "iAtM-lMmwMrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the code below and paste it into the file that is located on the path: /content/CCAM/CUSTOM/core/datasets.py"
      ],
      "metadata": {
        "id": "Xg6BQXERwPVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tools.ai.augment_utils import *\n",
        "\n",
        "\n",
        "class CUSTOM_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.image_name_list = []\n",
        "        for i in os.listdir(self.data_dir):\n",
        "            if i.endswith('.jpg') or i.endswith('.png'):\n",
        "                self.image_name_list.append(i)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_name_list)\n",
        "\n",
        "    def get_image(self, image_name):\n",
        "        image = Image.open(self.data_dir + image_name).convert('RGB')\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name = self.image_name_list[index]\n",
        "\n",
        "        image = cv2.imread(self.data_dir + image_name, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, image_name\n",
        "\n",
        "class CUSTOM_Dataset_For_Making_CAM(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.image_name_list = []\n",
        "        for i in os.listdir(self.data_dir):\n",
        "            if i.endswith('.jpg') or i.endswith('.png'):\n",
        "                self.image_name_list.append(i)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_name_list)\n",
        "\n",
        "    def get_image(self, image_name):\n",
        "        image = Image.open(self.data_dir + image_name).convert('RGB')\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name = self.image_name_list[index]\n",
        "\n",
        "        image = self.get_image(image_name)\n",
        "\n",
        "        return image, image_name"
      ],
      "metadata": {
        "id": "5Hy1ehu7wOrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train_CCAM.py"
      ],
      "metadata": {
        "id": "COSb27bVkffj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the code below and paste it into the file that is located on the path: /content/CCAM/CUSTOM/train_CCAM.py"
      ],
      "metadata": {
        "id": "8tL2LavUMqP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (C) 2020 * Ltd. All rights reserved.\n",
        "# author : Sanghyeon Jo <josanghyeokn@gmail.com>\n",
        "# modified by Sierkinhane <sierkinhane@163.com>\n",
        "\n",
        "import sys\n",
        "\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "from torchvision import transforms\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from utils import *\n",
        "from core.datasets import *\n",
        "from core.model import *\n",
        "from tools.general.io_utils import *\n",
        "from tools.general.time_utils import *\n",
        "from tools.general.json_utils import *\n",
        "from core.loss import *\n",
        "\n",
        "from tools.ai.log_utils import *\n",
        "from tools.ai.demo_utils import *\n",
        "from tools.ai.torch_utils import *\n",
        "from tools.ai.evaluate_utils import *\n",
        "\n",
        "from tools.ai.augment_utils import *\n",
        "from tools.ai.randaugment import *\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt\n",
        "from optimizer import PolyOptimizer\n",
        "from imgaug import augmenters as iaa\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"8\"\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "###############################################################################\n",
        "# Dataset\n",
        "###############################################################################\n",
        "parser.add_argument('--seed', default=0, type=int)\n",
        "parser.add_argument('--num_workers', default=8, type=int)\n",
        "parser.add_argument('--data_dir', default='/data1/xjheng/dataset/Market1501/bounding_box_train/', type=str)\n",
        "\n",
        "###############################################################################\n",
        "# Network\n",
        "###############################################################################\n",
        "parser.add_argument('--architecture', default='resnet50', type=str)\n",
        "parser.add_argument('--mode', default='normal', type=str)  # fix\n",
        "\n",
        "###############################################################################\n",
        "# Hyperparameter\n",
        "###############################################################################\n",
        "parser.add_argument('--batch_size', default=32, type=int)\n",
        "parser.add_argument('--max_epoch', default=10, type=int)\n",
        "parser.add_argument('--depth', default=50, type=int)\n",
        "\n",
        "parser.add_argument('--lr', default=0.001, type=float)\n",
        "parser.add_argument('--wd', default=1e-4, type=float)\n",
        "parser.add_argument('--nesterov', default=True, type=str2bool)\n",
        "\n",
        "parser.add_argument('--print_ratio', default=0.2, type=float)\n",
        "\n",
        "parser.add_argument('--tag', default='', type=str)\n",
        "parser.add_argument('--augment', default='', type=str)\n",
        "\n",
        "parser.add_argument('--alpha', type=float, default=0.25)\n",
        "parser.add_argument('--pretrained', type=str, required=True,\n",
        "                        help='adopt different pretrained parameters, [supervised, mocov2, detco]')\n",
        "\n",
        "flag = True\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # global flag\n",
        "    ###################################################################################\n",
        "    # Arguments\n",
        "    ###################################################################################\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    log_dir = create_directory('./experiments/logs/')\n",
        "    data_dir = create_directory('./experiments/data/')\n",
        "    model_dir = create_directory('./experiments/models/')\n",
        "    tensorboard_dir = create_directory('./experiments/tensorboards/{}/'.format(args.tag))\n",
        "\n",
        "    log_path = log_dir + '{}.txt'.format(args.tag)\n",
        "    data_path = data_dir + '{}.json'.format(args.tag)\n",
        "    model_path = model_dir + '{}.pth'.format(args.tag)\n",
        "    cam_path = './experiments/images/{}'.format(args.tag)\n",
        "    create_directory(cam_path)\n",
        "    create_directory(cam_path + '/train')\n",
        "    create_directory(cam_path + '/test')\n",
        "    create_directory(cam_path + '/train/colormaps')\n",
        "    create_directory(cam_path + '/test/colormaps')\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    log_func = lambda string='': log_print(string, log_path)\n",
        "\n",
        "    log_func('[i] {}'.format(args.tag))\n",
        "    log_func()\n",
        "\n",
        "    ###################################################################################\n",
        "    # Transform, Dataset, DataLoader\n",
        "    ###################################################################################\n",
        "    imagenet_mean = [0.485, 0.456, 0.406]\n",
        "    imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    normalize_fn = Normalize(imagenet_mean, imagenet_std)\n",
        "\n",
        "    # data augmentation\n",
        "    train_transform = transforms.Compose([\n",
        "        iaa.Sequential([\n",
        "            iaa.Fliplr(0.5),\n",
        "            iaa.Rot90((1, 3)),\n",
        "            iaa.Cutout(fill_mode=\"gaussian\", fill_per_channel=0.5, size=0.15),\n",
        "            iaa.Flipud(0.5),\n",
        "            iaa.Resize({\"height\": 224, \"width\": 224})\n",
        "        ]).augment_image,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    train_dataset = CUSTOM_Dataset(args.data_dir, train_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=True)\n",
        "\n",
        "    log_func('[i] mean values is {}'.format(imagenet_mean))\n",
        "    log_func('[i] std values is {}'.format(imagenet_std))\n",
        "    log_func('[i] train_transform is {}'.format(train_transform))\n",
        "    log_func('[i] train data is {}'.format(len(train_dataset)))\n",
        "    log_func()\n",
        "\n",
        "    val_iteration = len(train_loader)\n",
        "    log_iteration = int(val_iteration * args.print_ratio)\n",
        "    max_iteration = args.max_epoch * val_iteration\n",
        "\n",
        "    log_func('[i] log_iteration : {:,}'.format(log_iteration))\n",
        "    log_func('[i] val_iteration : {:,}'.format(val_iteration))\n",
        "    log_func('[i] max_iteration : {:,}'.format(max_iteration))\n",
        "\n",
        "    ###################################################################################\n",
        "    # Network\n",
        "    ###################################################################################\n",
        "    model = get_model(pretrained=args.pretrained)\n",
        "    param_groups = model.get_parameter_groups()\n",
        "\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "    # model_info(model)\n",
        "\n",
        "    log_func('[i] Architecture is {}'.format(args.architecture))\n",
        "    log_func('[i] Total Params: %.2fM' % (calculate_parameters(model)))\n",
        "    log_func()\n",
        "\n",
        "    try:\n",
        "        use_gpu = os.environ['CUDA_VISIBLE_DEVICES']\n",
        "    except KeyError:\n",
        "        use_gpu = '0'\n",
        "\n",
        "    the_number_of_gpu = len(use_gpu.split(','))\n",
        "    if the_number_of_gpu > 1:\n",
        "        log_func('[i] the number of gpu : {}'.format(the_number_of_gpu))\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    load_model_fn = lambda: load_model(model, model_path, parallel=the_number_of_gpu > 1)\n",
        "    save_model_fn = lambda: save_model(model, model_path, parallel=the_number_of_gpu > 1)\n",
        "    custom_model_path = 'models/' + '{}.pth'.format(args.tag)\n",
        "    custom_save_model_fn = lambda: save_model(model, custom_model_path, parallel=the_number_of_gpu > 1)\n",
        "\n",
        "    ###################################################################################\n",
        "    # Loss, Optimizer\n",
        "    ###################################################################################\n",
        "    criterion = [SimMaxLoss(metric='cos', alpha=args.alpha).cuda(), SimMinLoss(metric='cos').cuda(),\n",
        "                 SimMaxLoss(metric='cos', alpha=args.alpha).cuda()]\n",
        "\n",
        "    optimizer = PolyOptimizer([\n",
        "        {'params': param_groups[0], 'lr': args.lr, 'weight_decay': args.wd},\n",
        "        {'params': param_groups[1], 'lr': 2 * args.lr, 'weight_decay': 0},\n",
        "        {'params': param_groups[2], 'lr': 10 * args.lr, 'weight_decay': args.wd},\n",
        "        {'params': param_groups[3], 'lr': 20 * args.lr, 'weight_decay': 0},\n",
        "    ], lr=args.lr, momentum=0.9, weight_decay=args.wd, max_step=max_iteration)\n",
        "\n",
        "    #################################################################################################\n",
        "    # Train\n",
        "    #################################################################################################\n",
        "    data_dic = {\n",
        "        'train': [],\n",
        "        'validation': []\n",
        "    }\n",
        "\n",
        "    train_timer = Timer()\n",
        "    eval_timer = Timer()\n",
        "\n",
        "    train_meter = Average_Meter(['loss', 'positive_loss', 'negative_loss'])\n",
        "\n",
        "    epoch_train_meter = Average_Meter(['loss', 'positive_loss', 'negative_loss'])\n",
        "\n",
        "    writer = SummaryWriter(tensorboard_dir)\n",
        "\n",
        "    best_loss = 1000000000000\n",
        "\n",
        "    for epoch in range(args.max_epoch):\n",
        "        tracker = tqdm(train_loader, total=len(train_loader))\n",
        "        for iteration, (images, _) in enumerate(tracker):\n",
        "\n",
        "            images = images.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            fg_feats, bg_feats, ccam = model(images)\n",
        "\n",
        "            loss1 = criterion[0](fg_feats)\n",
        "            loss2 = criterion[1](bg_feats, fg_feats)\n",
        "            loss3 = criterion[2](bg_feats)\n",
        "\n",
        "            loss = loss1 + loss2 + loss3\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if epoch == 0 and iteration == 100:\n",
        "                flag = check_positive(ccam)\n",
        "                print(f\"Is Negative: {flag}\")\n",
        "            if flag:\n",
        "                ccam = 1 - ccam\n",
        "\n",
        "            train_meter.add({\n",
        "                'loss': loss.item(),\n",
        "                'positive_loss': loss1.item() + loss3.item(),\n",
        "                'negative_loss': loss2.item(),\n",
        "            })\n",
        "\n",
        "            epoch_train_meter.add({\n",
        "                'loss': loss.item(),\n",
        "                'positive_loss': loss1.item() + loss3.item(),\n",
        "                'negative_loss': loss2.item(),\n",
        "            })\n",
        "\n",
        "            #################################################################################################\n",
        "            # For Log\n",
        "            #################################################################################################\n",
        "            \n",
        "            # if (iteration + 1) % int(val_iteration/3) == 0:\n",
        "            if (iteration + 1) % 11 == 0:\n",
        "                #visualize_heatmap(args.tag, images.clone().detach(), ccam, 0, iteration)\n",
        "                loss, positive_loss, negative_loss = train_meter.get(clear=True)\n",
        "                learning_rate = float(get_learning_rate_from_optimizer(optimizer))\n",
        "\n",
        "                data = {\n",
        "                    'epoch': epoch,\n",
        "                    'max_epoch': args.max_epoch,\n",
        "                    'iteration': iteration + 1,\n",
        "                    'learning_rate': learning_rate,\n",
        "                    'loss': loss,\n",
        "                    'positive_loss': loss1,\n",
        "                    'negative_loss': loss2,\n",
        "\n",
        "                    'time': train_timer.tok(clear=True),\n",
        "                }\n",
        "                data_dic['train'].append(data)\n",
        "\n",
        "                log_func('[i]\\t'\n",
        "                         'Epoch[{epoch:,}/{max_epoch:,}],\\t'\n",
        "                         'iteration={iteration:,}, \\t'\n",
        "                         'learning_rate={learning_rate:.5f}, \\t'\n",
        "                         'loss={loss:.5f}, \\t'\n",
        "                         'positive_loss={positive_loss:.5f}, \\t'\n",
        "                         'negative_loss={negative_loss:.5f}, \\t'\n",
        "                         'time={time:.0f}sec'.format(**data)\n",
        "                         )\n",
        "\n",
        "                #writer.add_scalar('Train/loss', loss, iteration)\n",
        "                #writer.add_scalar('Train/learning_rate', learning_rate, iteration)\n",
        "        #################################################################################################\n",
        "        # Evaluation\n",
        "        #################################################################################################\n",
        "\n",
        "        loss, positive_loss, negative_loss = epoch_train_meter.get(clear=True)\n",
        "        if best_loss > loss:\n",
        "          best_loss = loss\n",
        "          print('Loss in epoch: ', loss)\n",
        "          save_model_fn()\n",
        "\n",
        "    print(args.tag)"
      ],
      "metadata": {
        "id": "GCGrVDmhovdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run train code"
      ],
      "metadata": {
        "id": "ps4grZY2kkRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description of argumentsв:\n",
        "\n",
        "*   `tag` - Name of the experiment.\n",
        "\n",
        "*   `max_epoch` - Number of epochs.\n",
        "\n",
        "*   `pretrained` - The argument takes 3 types of string: mocov2, detco и supervised. supervised.\n",
        "\n",
        "*   `alpha` - a hyperparameter that controls the smoothness of an exponential function. For more information, see point 3.3 in [paper](https://arxiv.org/pdf/2203.13505.pdf).\n",
        "\n",
        "*   `data_dir` - The path to the images. The folder should contain the images themselves, and not folders divided into the name of the classes in which the images are stored."
      ],
      "metadata": {
        "id": "zMYR_bTePn5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/CCAM/CUSTOM/train_CCAM.py --tag CCAM_partial_MOCO_224_4aug --max_epoch 140 --batch_size 256 --pretrained mocov2 --alpha 0.25 --data_dir 'train/partial/'\n",
        "#!python /content/CCAM/CUSTOM/train_CCAM.py --tag CCAM_drops_MOCO_224_4aug --max_epoch 25 --batch_size 128 --pretrained mocov2 --alpha 0.25 --data_dir 'train/drops/'\n",
        "!python /content/CCAM/CUSTOM/train_CCAM.py --tag CCAM_full_MOCO_224_4aug --max_epoch 25 --batch_size 128 --pretrained mocov2 --alpha 0.25 --data_dir 'train/full/'\n",
        "#!python /content/CCAM/CUSTOM/train_CCAM.py --tag CCAM_strong_MOCO_224_4aug --max_epoch 140 --batch_size 256 --pretrained mocov2 --alpha 0.25 --data_dir 'train/strong/'"
      ],
      "metadata": {
        "id": "8DMi96GiWvtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference CCAM"
      ],
      "metadata": {
        "id": "GsnBvVBXMfBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf vis_cam/\n",
        "!rm -rf experiments/predictions/"
      ],
      "metadata": {
        "id": "5k7qE3wQDLLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R /content/CCAM/WSSS/tools/ /content/\n",
        "!cp -R /content/CCAM/WSSS/core/ /content/\n",
        "!cp -R /content/CCAM/WSSS/inference_crf.py /content/\n",
        "!cp -R /content/CCAM/WSSS/optimizer.py /content/\n",
        "!cp -R /content/CCAM/WSSS/utils.py /content/\n",
        "!cp -R /content/CCAM/WSSS/inference_CCAM.py /content/"
      ],
      "metadata": {
        "id": "jmWLMiv7Mmc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add this class to the file that is located on the path: /content/core/datasets.py"
      ],
      "metadata": {
        "id": "2uLky-WBHpJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CUSTOM_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.image_name_list = []\n",
        "        for i in os.listdir(self.data_dir):\n",
        "            if i.endswith('.jpg') or i.endswith('.png'):\n",
        "                self.image_name_list.append(i)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_name_list)\n",
        "\n",
        "    def get_image(self, image_name):\n",
        "        image = Image.open(self.data_dir + image_name).convert('RGB')\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name = self.image_name_list[index]\n",
        "\n",
        "        image = self.get_image(image_name)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, image_name"
      ],
      "metadata": {
        "id": "vG8nQawPHoFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir experiments\n",
        "!mkdir experiments/models"
      ],
      "metadata": {
        "id": "o-27AOYAyCOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segmentation part"
      ],
      "metadata": {
        "id": "rRWGbLdS85qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1s7sFwfBp6vP1uURATChhuxC3Zvg1kcAT\n",
        "!gdown 1tIQGnL6_4ToXF1V3F4DQb0Uv2vb3wubr"
      ],
      "metadata": {
        "id": "P1tSZovv85qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir test_dataset\n",
        "!unzip images.zip -d /content/\n",
        "!unzip mask.zip -d /content/\n",
        "!mkdir test_dataset/{drops,partial,full,strong}"
      ],
      "metadata": {
        "id": "YZN6P3Jmsfe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def make_dataset(category):\n",
        "  root = \"\"\n",
        "  dir_name = \"mask/\" + category\n",
        "  for root, dirs, files in os.walk(os.path.join(root, dir_name)):\n",
        "    for file in files:\n",
        "      image_name = os.path.join(root, file)\n",
        "      os.replace(\"data/\" + image_name.split('/')[2], \"test_dataset/\" + category + \"/\" + image_name.split('/')[2])\n",
        "\n",
        "make_dataset(\"drops\")\n",
        "make_dataset(\"partial\")\n",
        "make_dataset(\"strong\")\n",
        "make_dataset(\"full\")"
      ],
      "metadata": {
        "id": "_LLHH9Ki09GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data\n",
        "!mkdir data\n",
        "!mv test_dataset images\n",
        "!mv images data/\n",
        "!mv mask data/"
      ],
      "metadata": {
        "id": "rl0PxOoxtirt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search Coefficient"
      ],
      "metadata": {
        "id": "dc3OZ431SDuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the code below and paste it into the file that is located on the path: /content/inference_CCAM.py"
      ],
      "metadata": {
        "id": "M5xfcvx1Od8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (C) 2020 * Ltd. All rights reserved.\n",
        "# author : Sanghyeon Jo <josanghyeokn@gmail.com>\n",
        "# modified by Sierkinhane <sierkinhane@163.com>\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import shutil\n",
        "import random\n",
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "# from torch.utils.tensorboardX import SummaryWriter\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from core.model import *\n",
        "from core.datasets import *\n",
        "\n",
        "from tools.general.io_utils import *\n",
        "from tools.general.time_utils import *\n",
        "from tools.general.json_utils import *\n",
        "\n",
        "from tools.ai.log_utils import *\n",
        "from tools.ai.demo_utils import *\n",
        "from tools.ai.optim_utils import *\n",
        "from tools.ai.torch_utils import *\n",
        "from tools.ai.evaluate_utils import *\n",
        "\n",
        "from tools.ai.augment_utils import *\n",
        "from tools.ai.randaugment import *\n",
        "\n",
        "from utils import check_positive\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "###############################################################################\n",
        "# Dataset\n",
        "###############################################################################\n",
        "parser.add_argument('--seed', default=0, type=int)\n",
        "parser.add_argument('--num_workers', default=8, type=int)\n",
        "parser.add_argument('--data_dir', default='/data1/xjheng/dataset/VOC2012/', type=str)\n",
        "\n",
        "###############################################################################\n",
        "# Network\n",
        "###############################################################################\n",
        "parser.add_argument('--architecture', default='resnet50', type=str)\n",
        "parser.add_argument('--mode', default='normal', type=str)\n",
        "\n",
        "parser.add_argument('--pretrained', type=str, required=True,\n",
        "                        help='adopt different pretrained parameters, [supervised, mocov2, detco]')\n",
        "\n",
        "###############################################################################\n",
        "# Inference parameters\n",
        "###############################################################################\n",
        "parser.add_argument('--tag', default='', type=str)\n",
        "parser.add_argument('--domain', default='train', type=str)\n",
        "parser.add_argument('--vis_dir', default='vis_cam', type=str)\n",
        "\n",
        "parser.add_argument('--scales', default='0.5,1.0,1.5,2.0', type=str)\n",
        "\n",
        "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
        "    outputs = outputs.int()\n",
        "    labels = labels.int()\n",
        "\n",
        "    SMOOTH = 1e-8\n",
        "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
        "    union = (outputs | labels).float().sum((1, 2))         # Will be zero if both are 0\n",
        "\n",
        "    iou = (torch.sum(intersection) + SMOOTH) / (torch.sum(union) + SMOOTH)  # We smooth(epsilon) our devision to avoid 0/0\n",
        "\n",
        "    return iou\n",
        "\n",
        "def get_masks(saliency_map, image_name, coefficient=10,resolution = 224, visualization=False):\n",
        "  hsv = cv2.cvtColor(saliency_map, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "  lower_blue = np.array([coefficient,50,50])\n",
        "  upper_blue = np.array([130,255,255])\n",
        "\n",
        "  mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "  \n",
        "  for i in range(len(mask)):\n",
        "    for j in range(len(mask[i])):\n",
        "      if mask[i][j] == 255: mask[i][j] = 0\n",
        "      else: mask[i][j] = 255\n",
        "\n",
        "  if visualization == True:\n",
        "    image = cv2.imread(image_name)\n",
        "    \n",
        "    img = image.copy()\n",
        "    img[mask == 255] = 0\n",
        "    color_channeled_image = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
        "    result = color_channeled_image * 0.4 + img * 0.7\n",
        "    mask_to_seg = saliency_map * 0.5 + image * 0.5\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
        "    ax1.set_title('Original picture')\n",
        "    ax2.set_title('The mask was created using CAM methods')\n",
        "    ax1.axis('off')\n",
        "    ax2.axis('off')\n",
        "    _ = ax1.imshow(cv2.cvtColor(mask_to_seg.astype(\"uint8\"), cv2.COLOR_BGR2RGB))\n",
        "    _ = ax2.imshow(cv2.cvtColor(result.astype(\"uint8\"), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  # Second argument - gives Grey Scale Image\n",
        "  real_mask = cv2.imread(\"data/mask/\" + category_classes[0] + \"/\" + image_name.split('.')[0] + \".png\", 0)\n",
        "  real_mask = cv2.resize(real_mask, (resolution, resolution), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "  for i in range(len(real_mask)):\n",
        "    for j in range(len(real_mask[i])):\n",
        "      if real_mask[i][j] > 0: real_mask[i][j] = 255\n",
        "      else: real_mask[i][j] = 0\n",
        "\n",
        "  #cv2.imwrite(\"paper/\" + image_name.split('_')[0] + \"/\" + \"mask_\" + image_name.split('.')[0] + \".png\", mask)\n",
        "  #cv2.imwrite(\"paper/\" + image_name.split('_')[0] + \"/\" + \"gt_\" + image_name.split('.')[0] + \".png\", real_mask)\n",
        "  #cv2.imwrite(\"paper/\" + image_name.split('_')[0] + \"/\" + \"map_\" + image_name.split('.')[0] + \".png\", saliency_map)\n",
        "  \n",
        "  real_mask = torch.Tensor(real_mask)\n",
        "  mask_tensor = torch.Tensor(mask)\n",
        "\n",
        "  return real_mask, mask_tensor\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    ###################################################################################\n",
        "    # Arguments\n",
        "    ###################################################################################\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    experiment_name = args.tag\n",
        "\n",
        "    resolution = int(experiment_name.split('_')[3])\n",
        "\n",
        "    category_classes = [experiment_name.split('_')[1]]\n",
        "\n",
        "    category_best_iou = {}\n",
        "    category_best_coefficient = {}\n",
        "\n",
        "    for class_name in category_classes:\n",
        "      category_best_iou[class_name] = 0\n",
        "      category_best_coefficient[class_name] = 0\n",
        "\n",
        "    if 'train' in args.domain:\n",
        "        experiment_name += '@train'\n",
        "    else:\n",
        "        experiment_name += '@val'\n",
        "\n",
        "    experiment_name += '@scale=%s'%args.scales\n",
        "    \n",
        "    pred_dir = create_directory(f'./experiments/predictions/{experiment_name}/')\n",
        "\n",
        "    cam_path = create_directory(f'{args.vis_dir}/{experiment_name}')\n",
        "\n",
        "    model_path = './experiments/models/' + f'{args.tag}.pth'\n",
        "    print(model_path)\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    log_func = lambda string='': print(string)\n",
        "\n",
        "    ###################################################################################\n",
        "    # Transform, Dataset, DataLoader\n",
        "    ###################################################################################\n",
        "    imagenet_mean = [0.485, 0.456, 0.406]\n",
        "    imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    normalize_fn = Normalize(imagenet_mean, imagenet_std)\n",
        "    \n",
        "    ###################################################################################\n",
        "    # Network\n",
        "    ###################################################################################\n",
        "    model = get_model(pretrained=args.pretrained)\n",
        "\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    log_func('[i] Architecture is {}'.format(args.architecture))\n",
        "    log_func('[i] Total Params: %.2fM'%(calculate_parameters(model)))\n",
        "    log_func()\n",
        "\n",
        "    try:\n",
        "        use_gpu = os.environ['CUDA_VISIBLE_DEVICES']\n",
        "    except KeyError:\n",
        "        use_gpu = '0'\n",
        "\n",
        "    the_number_of_gpu = len(use_gpu.split(','))\n",
        "    if the_number_of_gpu > 1:\n",
        "        log_func('[i] the number of gpu : {}'.format(the_number_of_gpu))\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    load_model(model, model_path, parallel=the_number_of_gpu > 1)\n",
        "    \n",
        "    #################################################################################################\n",
        "    # Evaluation\n",
        "    #################################################################################################\n",
        "    eval_timer = Timer()\n",
        "    scales = [float(scale) for scale in args.scales.split(',')]\n",
        "    \n",
        "    model.eval()\n",
        "    eval_timer.tik()\n",
        "\n",
        "    def get_cam(ori_image, scale):\n",
        "        # preprocessing\n",
        "        image = copy.deepcopy(ori_image)\n",
        "        image = image.resize((round(ori_w*scale), round(ori_h*scale)), resample=PIL.Image.CUBIC)\n",
        "        \n",
        "        image = normalize_fn(image)\n",
        "        image = image.transpose((2, 0, 1))\n",
        "\n",
        "        image = torch.from_numpy(image)\n",
        "        flipped_image = image.flip(-1)\n",
        "        \n",
        "        images = torch.stack([image, flipped_image])\n",
        "        images = images.cuda()\n",
        "        \n",
        "        # inferenece\n",
        "        _, _, cam = model(images, inference=True)\n",
        "        # flag = check_positive(cam.clone())\n",
        "        # if flag:\n",
        "        #     cam = 1 - cam\n",
        "        # postprocessing\n",
        "        cams = F.relu(cam)\n",
        "        # cams = torch.sigmoid(features)\n",
        "        cams = cams[0] + cams[1].flip(-1)\n",
        "\n",
        "        return cams\n",
        "\n",
        "    for coefficient in range(108,111):\n",
        "      print(\"Coefficient: \", coefficient)\n",
        "      for class_name in category_classes:\n",
        "        path_to_class = args.data_dir + class_name + '/'\n",
        "        dataset = CUSTOM_Dataset(path_to_class)\n",
        "        vis_cam = True\n",
        "        with torch.no_grad():\n",
        "          length = len(dataset)\n",
        "          gt_list = []\n",
        "          pred_list = []\n",
        "          for step, (ori_image, image_id) in enumerate(dataset):\n",
        "            image_file = copy.deepcopy(ori_image)\n",
        "            image_file = image_file.resize((resolution, resolution), resample=PIL.Image.CUBIC)\n",
        "            ori_w, ori_h = image_file.size\n",
        "            label = np.array([1])\n",
        "            npy_path = pred_dir + image_id + '.npy'\n",
        "            if os.path.isfile(npy_path):\n",
        "               continue\n",
        "            strided_size = get_strided_size((ori_h, ori_w), 4)\n",
        "            strided_up_size = get_strided_up_size((ori_h, ori_w), 16)\n",
        "\n",
        "            cams_list = [get_cam(image_file, scale) for scale in scales]\n",
        "\n",
        "            strided_cams_list = [resize_for_tensors(cams.unsqueeze(0), strided_size)[0] for cams in cams_list]\n",
        "            strided_cams = torch.sum(torch.stack(strided_cams_list), dim=0)\n",
        "            \n",
        "            hr_cams_list = [resize_for_tensors(cams.unsqueeze(0), strided_up_size)[0] for cams in cams_list]\n",
        "            hr_cams = torch.sum(torch.stack(hr_cams_list), dim=0)[:, :ori_h, :ori_w]\n",
        "            \n",
        "            keys = torch.nonzero(torch.from_numpy(label))[:, 0]\n",
        "            \n",
        "            strided_cams = strided_cams[keys]\n",
        "            strided_cams /= F.adaptive_max_pool2d(strided_cams, (1, 1)) + 1e-5\n",
        "            \n",
        "            hr_cams = hr_cams[keys]\n",
        "            hr_cams /= F.adaptive_max_pool2d(hr_cams, (1, 1)) + 1e-5\n",
        "\n",
        "            ######################################################################\n",
        "            cam = torch.sum(hr_cams, dim=0)\n",
        "            cam = cam.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "            cam = make_cam(cam).squeeze()\n",
        "            cam = get_numpy_from_tensor(cam)\n",
        "\n",
        "            image = np.array(image_file)\n",
        "\n",
        "            h, w, c = image.shape\n",
        "            \n",
        "            # type(cam) - numpy\n",
        "\n",
        "            cam = (cam * 255).astype(np.uint8)\n",
        "            #cam = np.uint8(255 * cam)\n",
        "            cam = cv2.resize(cam, (w, h), interpolation=cv2.INTER_LINEAR)\n",
        "            # cam = colormap(cam)\n",
        "            # cam = cv2.applyColorMap(cam,  cmapy.cmap('seismic')) # colormap function\n",
        "            cam = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
        "\n",
        "            cam = cv2.cvtColor(cam, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            #get_masks\n",
        "            gt_mask, pred_mask = get_masks(cam, image_id, coefficient=coefficient, resolution=resolution)\n",
        "\n",
        "            gt_list.append(gt_mask)\n",
        "            pred_list.append(pred_mask)\n",
        "\n",
        "            # 1 batch\n",
        "            #iou_metric = iou_pytorch(pred_mask.unsqueeze(0), gt_mask.unsqueeze(0))\n",
        "\n",
        "            #print(\"Image: \" + image_id + \" have IoU: \" + str(iou_metric.item()))\n",
        "\n",
        "            #image = cv2.addWeighted(image, 0.5, cam, 0.5, 0)\n",
        "            # if os.path.isfile(f'{cam_path}/{image_id}.png'):\n",
        "            #    continue\n",
        "            #cv2.imwrite(f'{cam_path}/{image_id}.png', image.astype(np.uint8))\n",
        "            ######################################################################\n",
        "            \n",
        "            # save cams\n",
        "            #keys = np.pad(keys + 1, (1, 0), mode='constant')\n",
        "            #np.save(npy_path, {\"keys\": keys, \"cam\": strided_cams.cpu(), \"hr_cam\": hr_cams.cpu().numpy()})\n",
        "            \n",
        "            #sys.stdout.write('\\r# Make CAM [{}/{}] = {:.2f}%, ({}, {})'.format(step + 1, length, (step + 1) / length * 100, (ori_h, ori_w), hr_cams.size()))\n",
        "            #sys.stdout.flush()\n",
        "        \n",
        "          gt_batch = torch.stack(gt_list)\n",
        "          pred_batch = torch.stack(pred_list)\n",
        "          iou = iou_pytorch(pred_batch, gt_batch).item()\n",
        "\n",
        "          if category_best_iou.get(class_name) < iou:\n",
        "            category_best_iou[class_name] = iou\n",
        "            category_best_coefficient[class_name] = coefficient\n",
        "        \n",
        "          #print()\n",
        "          #print(\"Category: \" + class_name + \" have IoU: \" + str(iou))\n",
        "    \n",
        "    if args.domain == 'train_aug':\n",
        "        args.domain = 'train'\n",
        "    \n",
        "    print(\"python3 inference_crf.py --experiment_name {} --domain {}\".format(experiment_name, args.domain))\n",
        "    print(\"category_best_iou = \", category_best_iou)\n",
        "    print(\"category_best_coefficient = \", category_best_coefficient)\n",
        "    \n",
        "    with open(args.tag + '_best_iou', 'w') as f: \n",
        "      json.dump(category_best_iou, f)\n",
        "\n",
        "    with open(args.tag + '_best_coefficient', 'w') as f: \n",
        "      json.dump(category_best_coefficient, f)"
      ],
      "metadata": {
        "id": "yZP4toV0Mi28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/inference_CCAM.py --tag CCAM_partial_MOCO_224_4aug --pretrained mocov2 --data_dir 'data/images/partial/' --domain 'val'\n",
        "#!python /content/inference_CCAM.py --tag CCAM_drops_MOCO_224_4aug --pretrained mocov2 --data_dir 'data/images/' --domain 'val'\n",
        "!python /content/inference_CCAM.py --tag CCAM_full_MOCO_224_4aug --pretrained mocov2 --data_dir 'data/images/' --domain 'val'\n",
        "#!python /content/inference_CCAM.py --tag CCAM_strong_MOCO_224_4aug --pretrained mocov2 --data_dir 'data/images/strong/' --domain 'val'"
      ],
      "metadata": {
        "id": "Aq9gwW0SBUFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make mask with best coefficient"
      ],
      "metadata": {
        "id": "2Dv7VN6WSSTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the code below and paste it into the file that is located on the path: /content/inference_CCAM.py"
      ],
      "metadata": {
        "id": "VSVmS3u1Sa79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (C) 2020 * Ltd. All rights reserved.\n",
        "# author : Sanghyeon Jo <josanghyeokn@gmail.com>\n",
        "# modified by Sierkinhane <sierkinhane@163.com>\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import shutil\n",
        "import random\n",
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "# from torch.utils.tensorboardX import SummaryWriter\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from core.model import *\n",
        "from core.datasets import *\n",
        "\n",
        "from tools.general.io_utils import *\n",
        "from tools.general.time_utils import *\n",
        "from tools.general.json_utils import *\n",
        "\n",
        "from tools.ai.log_utils import *\n",
        "from tools.ai.demo_utils import *\n",
        "from tools.ai.optim_utils import *\n",
        "from tools.ai.torch_utils import *\n",
        "from tools.ai.evaluate_utils import *\n",
        "\n",
        "from tools.ai.augment_utils import *\n",
        "from tools.ai.randaugment import *\n",
        "\n",
        "from utils import check_positive\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "###############################################################################\n",
        "# Dataset\n",
        "###############################################################################\n",
        "parser.add_argument('--seed', default=0, type=int)\n",
        "parser.add_argument('--num_workers', default=8, type=int)\n",
        "parser.add_argument('--data_dir', default='/data1/xjheng/dataset/VOC2012/', type=str)\n",
        "\n",
        "###############################################################################\n",
        "# Network\n",
        "###############################################################################\n",
        "parser.add_argument('--architecture', default='resnet50', type=str)\n",
        "parser.add_argument('--mode', default='normal', type=str)\n",
        "\n",
        "parser.add_argument('--pretrained', type=str, required=True,\n",
        "                        help='adopt different pretrained parameters, [supervised, mocov2, detco]')\n",
        "\n",
        "###############################################################################\n",
        "# Inference parameters\n",
        "###############################################################################\n",
        "parser.add_argument('--tag', default='', type=str)\n",
        "parser.add_argument('--domain', default='train', type=str)\n",
        "parser.add_argument('--vis_dir', default='vis_cam', type=str)\n",
        "\n",
        "parser.add_argument('--scales', default='0.5,1.0,1.5,2.0', type=str)\n",
        "\n",
        "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
        "    outputs = outputs.int()\n",
        "    labels = labels.int()\n",
        "\n",
        "    SMOOTH = 1e-8\n",
        "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
        "    union = (outputs | labels).float().sum((1, 2))         # Will be zero if both are 0\n",
        "\n",
        "    iou = (torch.sum(intersection) + SMOOTH) / (torch.sum(union) + SMOOTH)  # We smooth(epsilon) our devision to avoid 0/0\n",
        "\n",
        "    return iou\n",
        "\n",
        "def get_masks(saliency_map, image_name, coefficient=10, resolution = 224, path_to_dir = '', visualization=False):\n",
        "  hsv = cv2.cvtColor(saliency_map, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "  lower_blue = np.array([coefficient,50,50])\n",
        "  upper_blue = np.array([130,255,255])\n",
        "\n",
        "  mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "  \n",
        "  for i in range(len(mask)):\n",
        "    for j in range(len(mask[i])):\n",
        "      if mask[i][j] == 255: mask[i][j] = 0\n",
        "      else: mask[i][j] = 255\n",
        "\n",
        "  if visualization == True:\n",
        "    image = cv2.imread(image_name)\n",
        "    \n",
        "    img = image.copy()\n",
        "    img[mask == 255] = 0\n",
        "    color_channeled_image = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
        "    result = color_channeled_image * 0.4 + img * 0.7\n",
        "    mask_to_seg = saliency_map * 0.5 + image * 0.5\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
        "    ax1.set_title('Original picture')\n",
        "    ax2.set_title('The mask was created using CAM methods')\n",
        "    ax1.axis('off')\n",
        "    ax2.axis('off')\n",
        "    _ = ax1.imshow(cv2.cvtColor(mask_to_seg.astype(\"uint8\"), cv2.COLOR_BGR2RGB))\n",
        "    _ = ax2.imshow(cv2.cvtColor(result.astype(\"uint8\"), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  # Second argument - gives Grey Scale Image\n",
        "  real_mask = cv2.imread(\"data/mask/\" + category_classes[0] + \"/\" + image_name.split('.')[0] + \".png\", 0)\n",
        "  real_mask = cv2.resize(real_mask, (resolution, resolution), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "  for i in range(len(real_mask)):\n",
        "    for j in range(len(real_mask[i])):\n",
        "      if real_mask[i][j] > 0: real_mask[i][j] = 255\n",
        "      else: real_mask[i][j] = 0\n",
        "\n",
        "  path_for_class_dir = path_to_dir + '/' + category_classes[0]\n",
        "\n",
        "  try:\n",
        "    os.mkdir(path_for_class_dir)\n",
        "  except:\n",
        "    pass\n",
        "  \n",
        "  cv2.imwrite(path_for_class_dir + '/pred_' + image_name.split('.')[0] + '.png', mask)\n",
        "  cv2.imwrite(path_for_class_dir + '/gt_' + image_name.split('.')[0] + '.png', real_mask)\n",
        "  cv2.imwrite(path_for_class_dir + '/map_' + image_name.split('.')[0] + '.png', saliency_map)\n",
        "  \n",
        "  real_mask = torch.Tensor(real_mask)\n",
        "  mask_tensor = torch.Tensor(mask)\n",
        "\n",
        "  return real_mask, mask_tensor\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    ###################################################################################\n",
        "    # Arguments\n",
        "    ###################################################################################\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    experiment_name = args.tag\n",
        "\n",
        "    resolution = int(experiment_name.split('_')[3])\n",
        "\n",
        "    category_classes = [experiment_name.split('_')[1]]\n",
        "\n",
        "    with open(args.tag + '_best_coefficient', 'r') as f:\n",
        "      category_best_coefficient = json.load(f)\n",
        "\n",
        "    if 'train' in args.domain:\n",
        "        experiment_name += '@train'\n",
        "    else:\n",
        "        experiment_name += '@val'\n",
        "\n",
        "    experiment_name += '@scale=%s'%args.scales\n",
        "    \n",
        "    pred_dir = create_directory(f'./experiments/predictions/{experiment_name}/')\n",
        "\n",
        "    cam_path = create_directory(f'{args.vis_dir}/{experiment_name}')\n",
        "\n",
        "    model_path = './experiments/models/' + f'{args.tag}.pth'\n",
        "    print(model_path)\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    log_func = lambda string='': print(string)\n",
        "\n",
        "    ###################################################################################\n",
        "    # Transform, Dataset, DataLoader\n",
        "    ###################################################################################\n",
        "    imagenet_mean = [0.485, 0.456, 0.406]\n",
        "    imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    normalize_fn = Normalize(imagenet_mean, imagenet_std)\n",
        "    \n",
        "    ###################################################################################\n",
        "    # Network\n",
        "    ###################################################################################\n",
        "    model = get_model(pretrained=args.pretrained)\n",
        "\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    log_func('[i] Architecture is {}'.format(args.architecture))\n",
        "    log_func('[i] Total Params: %.2fM'%(calculate_parameters(model)))\n",
        "    log_func()\n",
        "\n",
        "    try:\n",
        "        use_gpu = os.environ['CUDA_VISIBLE_DEVICES']\n",
        "    except KeyError:\n",
        "        use_gpu = '0'\n",
        "\n",
        "    the_number_of_gpu = len(use_gpu.split(','))\n",
        "    if the_number_of_gpu > 1:\n",
        "        log_func('[i] the number of gpu : {}'.format(the_number_of_gpu))\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    load_model(model, model_path, parallel=the_number_of_gpu > 1)\n",
        "    \n",
        "    #################################################################################################\n",
        "    # Evaluation\n",
        "    #################################################################################################\n",
        "    eval_timer = Timer()\n",
        "    scales = [float(scale) for scale in args.scales.split(',')]\n",
        "    \n",
        "    model.eval()\n",
        "    eval_timer.tik()\n",
        "\n",
        "    def get_cam(ori_image, scale):\n",
        "        # preprocessing\n",
        "        image = copy.deepcopy(ori_image)\n",
        "        image = image.resize((round(ori_w*scale), round(ori_h*scale)), resample=PIL.Image.CUBIC)\n",
        "        \n",
        "        image = normalize_fn(image)\n",
        "        image = image.transpose((2, 0, 1))\n",
        "\n",
        "        image = torch.from_numpy(image)\n",
        "        flipped_image = image.flip(-1)\n",
        "        \n",
        "        images = torch.stack([image, flipped_image])\n",
        "        images = images.cuda()\n",
        "        \n",
        "        # inferenece\n",
        "        _, _, cam = model(images, inference=True)\n",
        "        # flag = check_positive(cam.clone())\n",
        "        # if flag:\n",
        "        #     cam = 1 - cam\n",
        "        # postprocessing\n",
        "        cams = F.relu(cam)\n",
        "        # cams = torch.sigmoid(features)\n",
        "        cams = cams[0] + cams[1].flip(-1)\n",
        "\n",
        "        return cams\n",
        "\n",
        "    for class_name in category_classes:\n",
        "      path_to_class = args.data_dir + class_name + '/'\n",
        "      dataset = CUSTOM_Dataset(path_to_class)\n",
        "      vis_cam = True\n",
        "      with torch.no_grad():\n",
        "        length = len(dataset)\n",
        "        gt_list = []\n",
        "        pred_list = []\n",
        "        for step, (ori_image, image_id) in enumerate(dataset):\n",
        "          image_file = copy.deepcopy(ori_image)\n",
        "          image_file = image_file.resize((resolution, resolution), resample=PIL.Image.CUBIC)\n",
        "          ori_w, ori_h = image_file.size\n",
        "          label = np.array([1])\n",
        "          npy_path = pred_dir + image_id + '.npy'\n",
        "          if os.path.isfile(npy_path):\n",
        "              continue\n",
        "          strided_size = get_strided_size((ori_h, ori_w), 4)\n",
        "          strided_up_size = get_strided_up_size((ori_h, ori_w), 16)\n",
        "\n",
        "          cams_list = [get_cam(image_file, scale) for scale in scales]\n",
        "\n",
        "          strided_cams_list = [resize_for_tensors(cams.unsqueeze(0), strided_size)[0] for cams in cams_list]\n",
        "          strided_cams = torch.sum(torch.stack(strided_cams_list), dim=0)\n",
        "          \n",
        "          hr_cams_list = [resize_for_tensors(cams.unsqueeze(0), strided_up_size)[0] for cams in cams_list]\n",
        "          hr_cams = torch.sum(torch.stack(hr_cams_list), dim=0)[:, :ori_h, :ori_w]\n",
        "          \n",
        "          keys = torch.nonzero(torch.from_numpy(label))[:, 0]\n",
        "          \n",
        "          strided_cams = strided_cams[keys]\n",
        "          strided_cams /= F.adaptive_max_pool2d(strided_cams, (1, 1)) + 1e-5\n",
        "          \n",
        "          hr_cams = hr_cams[keys]\n",
        "          hr_cams /= F.adaptive_max_pool2d(hr_cams, (1, 1)) + 1e-5\n",
        "\n",
        "          ######################################################################\n",
        "          cam = torch.sum(hr_cams, dim=0)\n",
        "          cam = cam.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "          cam = make_cam(cam).squeeze()\n",
        "          cam = get_numpy_from_tensor(cam)\n",
        "\n",
        "          image = np.array(image_file)\n",
        "\n",
        "          h, w, c = image.shape\n",
        "          \n",
        "          # type(cam) - numpy\n",
        "\n",
        "          #cv2.imwrite('cam-do-255.png', cam)\n",
        "\n",
        "          cam = (cam * 255).astype(np.uint8)\n",
        "          \n",
        "          #cv2.imwrite('paper/' + image_id.split('_')[0] + '/' + image_id.split('.')[0] + '.png', cam)\n",
        "          \n",
        "          #cam = np.uint8(255 * cam)\n",
        "          cam = cv2.resize(cam, (w, h), interpolation=cv2.INTER_LINEAR)\n",
        "          # cam = colormap(cam)\n",
        "          # cam = cv2.applyColorMap(cam,  cmapy.cmap('seismic')) # colormap function\n",
        "          cam = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
        "\n",
        "          cam = cv2.cvtColor(cam, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "          coefficient = category_best_coefficient.get(class_name)\n",
        "\n",
        "          #get_masks\n",
        "          gt_mask, pred_mask = get_masks(cam, image_id, coefficient=coefficient, resolution=resolution, path_to_dir = cam_path)\n",
        "\n",
        "          gt_list.append(gt_mask)\n",
        "          pred_list.append(pred_mask)\n",
        "\n",
        "          #print(\"Image name: \" + image_id + \" ,coefficient =  \" + str(coefficient) + \" , iou: \" + str(round(iou, 5)))\n",
        "      \n",
        "        gt_batch = torch.stack(gt_list)\n",
        "        pred_batch = torch.stack(pred_list)\n",
        "        iou = iou_pytorch(pred_batch, gt_batch).item()\n",
        "\n",
        "        #print(\"Image name: \" + image_id + \" ,coefficient =  \" + str(coefficient) + \" , iou: \" + str(round(iou, 5)))\n",
        "    \n",
        "    if args.domain == 'train_aug':\n",
        "        args.domain = 'train'\n",
        "    \n",
        "    print(\"python3 inference_crf.py --experiment_name {} --domain {}\".format(experiment_name, args.domain))"
      ],
      "metadata": {
        "id": "G2X3dRcvSa79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/inference_CCAM.py --tag CCAM_partial_MOCO_224_4aug --pretrained mocov2 --data_dir 'data/images/partial/' --domain 'val'\n",
        "#!python /content/inference_CCAM.py --tag CCAM_drops_MOCO_224_4aug --pretrained mocov2 --data_dir 'data/images/' --domain 'val'\n",
        "!python /content/inference_CCAM.py --tag CCAM_full_MOCO_224_4aug --pretrained mocov2 --data_dir 'data/images/' --domain 'val'\n",
        "#!python /content/inference_CCAM.py --tag CCAM_strong_MOCO_224_4aug --pretrained mocov2 --data_dir 'data/images/strong/' --domain 'val'"
      ],
      "metadata": {
        "id": "rAsYgef6CZXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Results"
      ],
      "metadata": {
        "id": "LsyGsaEV-f4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rename the folder for convenient operation"
      ],
      "metadata": {
        "id": "jgdOp-37PdAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 'vis_cam/CCAM_full_MOCO_224_4aug@val@scale=0.5,1.0,1.5,2.0/' 'vis_cam/CCAM_full_MOCO/'"
      ],
      "metadata": {
        "id": "hWmz83qAbdZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a zip file for easy saving of the results"
      ],
      "metadata": {
        "id": "GEI1jpxDb2T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r vis_cam_drops_and_full.zip vis_cam/"
      ],
      "metadata": {
        "id": "RIV2PkMAQMf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "uV5LTi-O4ef5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv vis_cam_drops_and_full.zip gdrive/MyDrive/CaUS/"
      ],
      "metadata": {
        "id": "P-aAFzqr4oPg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}