{
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RQ5pQ4fBURW2",
        "w5QVUjxmgZqT",
        "DhgfAj4ubmFt",
        "5sfaFpawR95y",
        "bePuQDjsXQon",
        "VZOKovNrYGjF",
        "Gub8IloPYB2s",
        "lYlZLjvv4Lq7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Important moment"
      ],
      "metadata": {
        "id": "RQ5pQ4fBURW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serviceability was checked in Google Colab."
      ],
      "metadata": {
        "id": "HCiHjXTjUWRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the resolution on which the classification model used was trained."
      ],
      "metadata": {
        "id": "8mWu4sKvURW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resolution_model = 224"
      ],
      "metadata": {
        "id": "LTbZkmrlURW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Drive and prepare Data"
      ],
      "metadata": {
        "id": "w5QVUjxmgZqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should use gpu to get results quickly."
      ],
      "metadata": {
        "id": "UaXGuYNrD5YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "K6coTGNGNLz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to have the following structure so that you don't have to change anything in the code for working with data."
      ],
      "metadata": {
        "id": "GahGw2uvltFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1s7sFwfBp6vP1uURATChhuxC3Zvg1kcAT\n",
        "!gdown 1tIQGnL6_4ToXF1V3F4DQb0Uv2vb3wubr"
      ],
      "metadata": {
        "id": "P1tSZovv85qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir test_dataset\n",
        "!unzip images.zip -d /content/\n",
        "!unzip mask.zip -d /content/\n",
        "!mkdir test_dataset/{drops,partial,full,strong}"
      ],
      "metadata": {
        "id": "YZN6P3Jmsfe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def make_dataset(category):\n",
        "  root = \"\"\n",
        "  dir_name = \"mask/\" + category\n",
        "  for root, dirs, files in os.walk(os.path.join(root, dir_name)):\n",
        "    for file in files:\n",
        "      image_name = os.path.join(root, file)\n",
        "      os.replace(\"data/\" + image_name.split('/')[2], \"test_dataset/\" + category + \"/\" + image_name.split('/')[2])\n",
        "\n",
        "make_dataset(\"drops\")\n",
        "make_dataset(\"partial\")\n",
        "make_dataset(\"strong\")\n",
        "make_dataset(\"full\")"
      ],
      "metadata": {
        "id": "_LLHH9Ki09GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data\n",
        "!mkdir data\n",
        "!mv test_dataset images\n",
        "!mv images data/\n",
        "!mv mask data/"
      ],
      "metadata": {
        "id": "rl0PxOoxtirt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the libraries and import them"
      ],
      "metadata": {
        "id": "DhgfAj4ubmFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "!pip install grad-cam"
      ],
      "metadata": {
        "id": "rv-mIj_GEVU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import json\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torchvision.models import resnet18 as resnet18\n",
        "from torchvision import models, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import timm\n",
        "\n",
        "from PIL import Image\n",
        "import random\n",
        "import pickle\n",
        "import io\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "a67b00bd-5509-4b62-84ad-f8bcb5dd975f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CAMERAS"
      ],
      "metadata": {
        "id": "5sfaFpawR95y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VisMIL/CAMERAS.git\n",
        "!cp -R /content/CAMERAS/CAMERAS.py /content/"
      ],
      "metadata": {
        "id": "EN-foQlQSBjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the line self.inputResolutions = list(range(**224**, 1000, 100)) - change the underlined number to the one that the classification model was trained for (must match what is set in \"Important moment\").\n",
        "\n",
        "Copy the code below and paste it into the file that is located on the path: /content/CAMERAS.py"
      ],
      "metadata": {
        "id": "S8HiSiPr2LKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CAMERAS():\n",
        "    def __init__(self, model, targetLayerName, inputResolutions=None):\n",
        "        self.model = model\n",
        "        self.inputResolutions = inputResolutions\n",
        "\n",
        "        if self.inputResolutions is None:\n",
        "            self.inputResolutions = list(range(224, 1000, 100)) # Update this line\n",
        "\n",
        "        self.classDict = {}\n",
        "        self.probsDict = {}\n",
        "        self.featureDict = {}\n",
        "        self.gradientsDict = {}\n",
        "        self.targetLayerName = targetLayerName\n",
        "\n",
        "    def _recordActivationsAndGradients(self, inputResolution, image, classOfInterest=None):\n",
        "        def forward_hook(module, input, output):\n",
        "            self.featureDict[inputResolution] = (copy.deepcopy(output.clone().detach().cpu()))\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradientsDict[inputResolution] = (copy.deepcopy(grad_output[0].clone().detach().cpu()))\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if name == self.targetLayerName:\n",
        "                forwardHandle = module.register_forward_hook(forward_hook)\n",
        "                backwardHandle = module.register_backward_hook(backward_hook)\n",
        "\n",
        "        logits = self.model(image)\n",
        "        softMaxScore = F.softmax(logits, dim=1)\n",
        "        probs, classes = softMaxScore.sort(dim=1, descending=True)\n",
        "\n",
        "        if classOfInterest is None:\n",
        "            ids = classes[:, [0]]\n",
        "        else:\n",
        "            ids = torch.tensor(classOfInterest).unsqueeze(dim=0).unsqueeze(dim=0).cuda()\n",
        "\n",
        "        self.classDict[inputResolution] = ids.clone().detach().item()\n",
        "        self.probsDict[inputResolution] = probs[0, 0].clone().detach().item()\n",
        "\n",
        "        one_hot = torch.zeros_like(logits)\n",
        "        one_hot.scatter_(1, ids, 1.0)\n",
        "\n",
        "        self.model.zero_grad()\n",
        "        logits.backward(gradient=one_hot, retain_graph=False)\n",
        "        forwardHandle.remove()\n",
        "        backwardHandle.remove()\n",
        "        del forward_hook\n",
        "        del backward_hook\n",
        "\n",
        "    def _estimateSaliencyMap(self, classOfInterest):\n",
        "        saveResolution = self.inputResolutions[0]\n",
        "        groundTruthClass = self.classDict[saveResolution]\n",
        "        meanScaledFeatures = None\n",
        "        meanScaledGradients = None\n",
        "\n",
        "        count = 0\n",
        "        for resolution in self.inputResolutions:\n",
        "            if groundTruthClass == self.classDict[resolution] or self.classDict[resolution] == classOfInterest:\n",
        "                count += 1\n",
        "                upSampledFeatures = F.interpolate(self.featureDict[resolution].cuda(), (saveResolution, saveResolution), mode='bilinear', align_corners=False)\n",
        "                upSampledGradients = F.interpolate(self.gradientsDict[resolution].cuda(), (saveResolution, saveResolution), mode='bilinear', align_corners=False)\n",
        "\n",
        "                if meanScaledFeatures is None:\n",
        "                    meanScaledFeatures = upSampledFeatures\n",
        "                else:\n",
        "                    meanScaledFeatures += upSampledFeatures\n",
        "\n",
        "                if meanScaledGradients is None:\n",
        "                    meanScaledGradients = upSampledGradients\n",
        "                else:\n",
        "                    meanScaledGradients += upSampledGradients\n",
        "\n",
        "        meanScaledFeatures /= count\n",
        "        meanScaledGradients /= count\n",
        "\n",
        "        fmaps = meanScaledFeatures\n",
        "        grads = meanScaledGradients\n",
        "\n",
        "        saliencyMap = torch.mul(fmaps, grads).sum(dim=1, keepdim=True)\n",
        "\n",
        "        saliencyMap = F.relu(saliencyMap)\n",
        "        B, C, H, W = saliencyMap.shape\n",
        "        saliencyMap = saliencyMap.view(B, -1)\n",
        "        saliencyMap -= saliencyMap.min(dim=1, keepdim=True)[0]\n",
        "        saliencyMap /= saliencyMap.max(dim=1, keepdim=True)[0]\n",
        "        saliencyMap = saliencyMap.view(B, C, H, W)\n",
        "\n",
        "        saliencyMap = torch.squeeze(torch.squeeze(saliencyMap, dim=0), dim=0)\n",
        "        return saliencyMap\n",
        "\n",
        "    def run(self, image, classOfInterest=None):\n",
        "        for index, inputResolution in enumerate(self.inputResolutions):\n",
        "            if index == 0:\n",
        "                upSampledImage = image.cuda()\n",
        "            else:\n",
        "                upSampledImage = F.interpolate(image, (inputResolution, inputResolution), mode='bicubic', align_corners=False).cuda()\n",
        "\n",
        "            self._recordActivationsAndGradients(inputResolution, upSampledImage, classOfInterest=classOfInterest)\n",
        "\n",
        "        saliencyMap = self._estimateSaliencyMap(classOfInterest=classOfInterest)\n",
        "        return saliencyMap, self.classDict, self.probsDict"
      ],
      "metadata": {
        "id": "DT6Prri4XMDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell does not need to be copied, just run"
      ],
      "metadata": {
        "id": "iDLOmEOPF6JI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import matplotlib.cm as cm\n",
        "from CAMERAS import CAMERAS\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "normalizeTransform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "normalizeImageTransform = transforms.Compose([transforms.ToTensor(), normalizeTransform])\n",
        "\n",
        "def loadImage(imagePath, imageSize):\n",
        "    rawImage = cv2.imread(imagePath)\n",
        "    rawImage = cv2.resize(rawImage, (resolution_model,) * 2, interpolation=cv2.INTER_LINEAR)\n",
        "    rawImage = cv2.resize(rawImage, (imageSize,) * 2, interpolation=cv2.INTER_LINEAR)\n",
        "    image = normalizeImageTransform(rawImage[..., ::-1].copy())\n",
        "    return image, rawImage\n",
        "\n",
        "def saveMapWithColorMap(filename, map, image):\n",
        "    cmap = cv2.applyColorMap(np.uint8(255 * map.detach().numpy().squeeze()), cv2.COLORMAP_JET)\n",
        "    map = (cmap.astype(np.float) + image.astype(np.float)) / 2\n",
        "    cv2.imwrite(filename, np.uint8(map))\n",
        "\n",
        "def computeAndSaveMaps(model, image_name):\n",
        "    cameras = CAMERAS(model, targetLayerName=\"layer4\") # classic CAMERAS\n",
        "\n",
        "    image, rawImage = loadImage(image_name, imageSize=resolution_model)\n",
        "    image = torch.unsqueeze(image, dim=0)\n",
        "\n",
        "    saliencyMap, predicted_class, probs_predict = cameras.run(image)\n",
        "    saliencyMap = saliencyMap.cpu()\n",
        "\n",
        "    return saliencyMap"
      ],
      "metadata": {
        "id": "5FvIxemxSoHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of all CAM methods used in the article, except CAMERAS and CCAM"
      ],
      "metadata": {
        "id": "bePuQDjsXQon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mGrad-CAM"
      ],
      "metadata": {
        "id": "VZOKovNrYGjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cells does not need to be copied, just run"
      ],
      "metadata": {
        "id": "Ik8BGFMIGUi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam.utils.image import show_cam_on_image, deprocess_image, preprocess_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
      ],
      "metadata": {
        "id": "o0S1V7oKYsVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import ttach as tta\n",
        "from typing import Callable, List, Tuple\n",
        "from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n",
        "from pytorch_grad_cam.utils.svd_on_activations import get_2d_projection\n",
        "from pytorch_grad_cam.utils.image import scale_cam_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "\n",
        "class BaseCAM:\n",
        "    def __init__(self,\n",
        "                 model: torch.nn.Module,\n",
        "                 target_layers: List[torch.nn.Module],\n",
        "                 use_cuda: bool = False,\n",
        "                 reshape_transform: Callable = None,\n",
        "                 compute_input_gradient: bool = False,\n",
        "                 uses_gradients: bool = True) -> None:\n",
        "        self.model = model.eval()\n",
        "        self.target_layers = target_layers\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "        self.reshape_transform = reshape_transform\n",
        "        self.compute_input_gradient = compute_input_gradient\n",
        "        self.uses_gradients = uses_gradients\n",
        "        self.activations_and_grads = ActivationsAndGradients(\n",
        "            self.model, target_layers, reshape_transform)\n",
        "\n",
        "    \"\"\" Get a vector of weights for every channel in the target layer.\n",
        "        Methods that return weights channels,\n",
        "        will typically need to only implement this function. \"\"\"\n",
        "\n",
        "    def get_cam_weights(self,\n",
        "                        input_tensor: torch.Tensor,\n",
        "                        target_layers: List[torch.nn.Module],\n",
        "                        targets: List[torch.nn.Module],\n",
        "                        activations: torch.Tensor,\n",
        "                        grads: torch.Tensor) -> np.ndarray:\n",
        "        raise Exception(\"Not Implemented\")\n",
        "\n",
        "    def get_cam_image(self,\n",
        "                      input_tensor: torch.Tensor,\n",
        "                      target_layer: torch.nn.Module,\n",
        "                      targets: List[torch.nn.Module],\n",
        "                      activations: torch.Tensor,\n",
        "                      grads: torch.Tensor,\n",
        "                      eigen_smooth: bool = False) -> np.ndarray:\n",
        "\n",
        "        weights = self.get_cam_weights(input_tensor,\n",
        "                                       target_layer,\n",
        "                                       targets,\n",
        "                                       activations,\n",
        "                                       grads)\n",
        "        weighted_activations = weights * activations\n",
        "        if eigen_smooth:\n",
        "            cam = get_2d_projection(weighted_activations)\n",
        "        else:\n",
        "            cam = weighted_activations.sum(axis=1)\n",
        "        return cam\n",
        "\n",
        "    def forward(self,\n",
        "                input_tensor: torch.Tensor,\n",
        "                targets: List[torch.nn.Module],\n",
        "                eigen_smooth: bool = False) -> np.ndarray:\n",
        "\n",
        "        if self.cuda:\n",
        "            input_tensor = input_tensor.cuda()\n",
        "\n",
        "        if self.compute_input_gradient:\n",
        "            input_tensor = torch.autograd.Variable(input_tensor,\n",
        "                                                   requires_grad=True)\n",
        "\n",
        "        outputs = self.activations_and_grads(input_tensor)\n",
        "        if targets is None:\n",
        "            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
        "            targets = [ClassifierOutputTarget(category) for category in target_categories]\n",
        "\n",
        "        if self.uses_gradients:\n",
        "            self.model.zero_grad()\n",
        "            loss = sum([target(output) for target, output in zip(targets, outputs)])\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "        # In most of the saliency attribution papers, the saliency is\n",
        "        # computed with a single target layer.\n",
        "        # Commonly it is the last convolutional layer.\n",
        "        # Here we support passing a list with multiple target layers.\n",
        "        # It will compute the saliency image for every image,\n",
        "        # and then aggregate them (with a default mean aggregation).\n",
        "        # This gives you more flexibility in case you just want to\n",
        "        # use all conv layers for example, all Batchnorm layers,\n",
        "        # or something else.\n",
        "        cam_per_layer = self.compute_cam_per_layer(input_tensor,\n",
        "                                                   targets,\n",
        "                                                   eigen_smooth)\n",
        "        return self.aggregate_multi_layers(cam_per_layer)\n",
        "\n",
        "    def get_target_width_height(self,\n",
        "                                input_tensor: torch.Tensor) -> Tuple[int, int]:\n",
        "        width, height = input_tensor.size(-1), input_tensor.size(-2)\n",
        "        return width, height\n",
        "\n",
        "    def compute_cam_per_layer(\n",
        "            self,\n",
        "            input_tensor: torch.Tensor,\n",
        "            targets: List[torch.nn.Module],\n",
        "            eigen_smooth: bool) -> np.ndarray:\n",
        "        activations_list = [a.cpu().data.numpy()\n",
        "                            for a in self.activations_and_grads.activations]\n",
        "        grads_list = [g.cpu().data.numpy()\n",
        "                      for g in self.activations_and_grads.gradients]\n",
        "        target_size = self.get_target_width_height(input_tensor)\n",
        "\n",
        "        cam_per_target_layer = []\n",
        "        # Loop over the saliency image from every layer\n",
        "        for i in range(len(self.target_layers)):\n",
        "            target_layer = self.target_layers[i]\n",
        "            layer_activations = None\n",
        "            layer_grads = None\n",
        "            if i < len(activations_list):\n",
        "                layer_activations = activations_list[i]\n",
        "            if i < len(grads_list):\n",
        "                layer_grads = grads_list[i]\n",
        "\n",
        "            cam = self.get_cam_image(input_tensor,\n",
        "                                     target_layer,\n",
        "                                     targets,\n",
        "                                     layer_activations,\n",
        "                                     layer_grads,\n",
        "                                     eigen_smooth)\n",
        "            cam = np.maximum(cam, 0)\n",
        "            scaled = scale_cam_image(cam, target_size)\n",
        "            cam_per_target_layer.append(scaled[:, None, :])\n",
        "\n",
        "        return cam_per_target_layer\n",
        "\n",
        "    def aggregate_multi_layers(self, cam_per_target_layer: np.ndarray) -> np.ndarray:\n",
        "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
        "        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n",
        "        result = np.mean(cam_per_target_layer, axis=1)\n",
        "        return scale_cam_image(result)\n",
        "\n",
        "    def forward_augmentation_smoothing(self,\n",
        "                                       input_tensor: torch.Tensor,\n",
        "                                       targets: List[torch.nn.Module],\n",
        "                                       eigen_smooth: bool = False) -> np.ndarray:\n",
        "        transforms = tta.Compose(\n",
        "            [\n",
        "                tta.HorizontalFlip(),\n",
        "                tta.Multiply(factors=[0.9, 1, 1.1]),\n",
        "            ]\n",
        "        )\n",
        "        cams = []\n",
        "        for transform in transforms:\n",
        "            augmented_tensor = transform.augment_image(input_tensor)\n",
        "            cam = self.forward(augmented_tensor,\n",
        "                               targets,\n",
        "                               eigen_smooth)\n",
        "\n",
        "            # The ttach library expects a tensor of size BxCxHxW\n",
        "            cam = cam[:, None, :, :]\n",
        "            cam = torch.from_numpy(cam)\n",
        "            cam = transform.deaugment_mask(cam)\n",
        "\n",
        "            # Back to numpy float32, HxW\n",
        "            cam = cam.numpy()\n",
        "            cam = cam[:, 0, :, :]\n",
        "            cams.append(cam)\n",
        "\n",
        "        cam = np.mean(np.float32(cams), axis=0)\n",
        "        return cam\n",
        "\n",
        "    def __call__(self,\n",
        "                 input_tensor: torch.Tensor,\n",
        "                 targets: List[torch.nn.Module] = None,\n",
        "                 aug_smooth: bool = False,\n",
        "                 eigen_smooth: bool = False) -> np.ndarray:\n",
        "\n",
        "        # Smooth the CAM result with test time augmentation\n",
        "        if aug_smooth is True:\n",
        "            return self.forward_augmentation_smoothing(\n",
        "                input_tensor, targets, eigen_smooth)\n",
        "\n",
        "        return self.forward(input_tensor,\n",
        "                            targets, eigen_smooth)\n",
        "\n",
        "    def __del__(self):\n",
        "        self.activations_and_grads.release()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
        "        self.activations_and_grads.release()\n",
        "        if isinstance(exc_value, IndexError):\n",
        "            # Handle IndexError here...\n",
        "            print(\n",
        "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
        "            return True"
      ],
      "metadata": {
        "id": "r-yCEJCykmBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GradCAM(BaseCAM):\n",
        "    def __init__(self, model, target_layers, use_cuda=False,\n",
        "                 reshape_transform=None):\n",
        "        super(\n",
        "            GradCAM,\n",
        "            self).__init__(\n",
        "            model,\n",
        "            target_layers,\n",
        "            use_cuda,\n",
        "            reshape_transform)\n",
        "\n",
        "    def get_cam_weights(self,\n",
        "                        input_tensor,\n",
        "                        target_layer,\n",
        "                        target_category,\n",
        "                        activations,\n",
        "                        grads):\n",
        "        return grads"
      ],
      "metadata": {
        "id": "f1_IzLuOkv_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LayerCAM, Grad-CAM, FullGrad and others"
      ],
      "metadata": {
        "id": "Gub8IloPYB2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell does not need to be copied, just run"
      ],
      "metadata": {
        "id": "iDS2ExJbGWY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, LayerCAM, FullGrad\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, deprocess_image, preprocess_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
      ],
      "metadata": {
        "id": "NAyAkLOgFojp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised segmentation pipeline"
      ],
      "metadata": {
        "id": "lYlZLjvv4Lq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CAMERAS:\n",
        "\n",
        "To use the CAMERAS method, you need to follow all the instructions from the text and activate all the cells in the section \"CAMERAS\".\n",
        "\n",
        "---\n",
        "\n",
        "Grad-CAM, mGrad-CAM and etc:\n",
        "\n",
        "To use mGrad-CAM -> first go to the section \"Implementation of all CAM methods used in the article, except CAMERAS and CCAM\", go to the subsection \"mGrad-CAM\" and activate all the cells, then go back to this section and check in the function \"get_saliency_map\" set GradCAM.\n",
        "\n",
        "---\n",
        "\n",
        "To use other methods (the exceptions are CAMERAS and mGrad-CAM) -> first go to the section \"Implementation of all CAM methods used in the article, except CAMERAS and CCAM\", go to the subsection \"LayerCAM, Grad-CAM, FullGrad and others\" execute a single cell, then return to this section and set in the function \"get_saliency_map\" which CAM will we use.\n",
        "\n",
        "---\n",
        "\n",
        "If you change models every time, for example in this sequence::\n",
        "\n",
        "1. Grad-CAM;\n",
        "2. mGrad-CAM;\n",
        "3. Layer-CAM.\n",
        "\n",
        "Then be sure to do what is described above every time and for full confidence you can restart the environment.\n",
        "\n",
        "If there is no mGrad-CAM in the sequence, then it is enough to do it once and then change the CAM method in get_saliency_map."
      ],
      "metadata": {
        "id": "Fv4CnSpx40Ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.layerN[-1] - the last layer in the block;\n",
        "\n",
        "model.layerN[-2] - the first layer in the block."
      ],
      "metadata": {
        "id": "aWFqRT_PW8Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_saliency_map(model, image_name):\n",
        "  # From which layers will the class activation maps be taken\n",
        "  target_layers = [model.layer3[-1], model.layer4[-2]]\n",
        "\n",
        "  normalize = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225]\n",
        "  )\n",
        "\n",
        "  preprocess = transforms.Compose([\n",
        "    transforms.Resize((resolution_model, resolution_model)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "  ])\n",
        "\n",
        "  # load test image\n",
        "  img_pil = Image.open(image_name).convert('RGB')\n",
        "\n",
        "  img_tensor = preprocess(img_pil)\n",
        "  img_variable = Variable(img_tensor.unsqueeze(0))\n",
        "\n",
        "  input_tensor = img_variable.cuda()\n",
        "\n",
        "  # This specifies which CAM method will be used\n",
        "  cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
        "\n",
        "  grayscale_cam = cam(input_tensor=input_tensor)\n",
        "  # In this example grayscale_cam has only one image in the batch:\n",
        "  grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "  return grayscale_cam"
      ],
      "metadata": {
        "id": "VbkieVOK4T-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
        "    outputs = outputs.int()\n",
        "    labels = labels.int()\n",
        "\n",
        "    SMOOTH = 1e-8\n",
        "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
        "    union = (outputs | labels).float().sum((1, 2))         # Will be zero if both are 0\n",
        "\n",
        "    iou = (torch.sum(intersection) + SMOOTH) / (torch.sum(union) + SMOOTH)  # We smooth(epsilon) our devision to avoid 0/0\n",
        "\n",
        "    return iou"
      ],
      "metadata": {
        "id": "9jWoaXMw6otM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_masks(saliency_map, path_to_image, coefficient = 10, resolution = 224, visualization = False):\n",
        "  if not isinstance(saliency_map, np.ndarray):\n",
        "    saliency_map = cv2.applyColorMap(np.uint8(255 * saliency_map.detach().numpy().squeeze()), cv2.COLORMAP_JET)\n",
        "  else:\n",
        "    saliency_map = cv2.applyColorMap(np.uint8(255 * saliency_map), cv2.COLORMAP_JET)\n",
        "  \n",
        "  hsv = cv2.cvtColor(saliency_map, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "  # Here we define the blue color range in HSV\n",
        "  lower_blue = np.array([coefficient,50,50])\n",
        "  upper_blue = np.array([130,255,255])\n",
        "\n",
        "  # This method creates a blue mask of the objects found in the frame\n",
        "  mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "  \n",
        "  # Inverting the mask\n",
        "  for i in range(len(mask)):\n",
        "    for j in range(len(mask[i])):\n",
        "      if mask[i][j] == 255: mask[i][j] = 0\n",
        "      else: mask[i][j] = 255\n",
        "\n",
        "  if visualization == True:\n",
        "    image = cv2.imread(path_to_image)\n",
        "    \n",
        "    img = image.copy()\n",
        "    img[mask == 255] = 0\n",
        "    color_channeled_image = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
        "    result = color_channeled_image * 0.4 + img * 0.7\n",
        "    mask_to_seg = saliency_map * 0.5 + image * 0.5\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
        "    ax1.set_title('Original picture')\n",
        "    ax2.set_title('The mask was created using CAM methods')\n",
        "    ax1.axis('off')\n",
        "    ax2.axis('off')\n",
        "    _ = ax1.imshow(cv2.cvtColor(mask_to_seg.astype(\"uint8\"), cv2.COLOR_BGR2RGB))\n",
        "    _ = ax2.imshow(cv2.cvtColor(result.astype(\"uint8\"), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  file_name = path_to_image.split('/')[-1]\n",
        "  # Second argument - gives Grey Scale Image\n",
        "  real_mask = cv2.imread(\"data/mask/\"+ path_to_image.split('/')[2] + \"/\" + file_name.split('.')[0] + \".png\", 0)\n",
        "  real_mask = cv2.resize(real_mask, (resolution, resolution), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "  for i in range(len(real_mask)):\n",
        "    for j in range(len(real_mask[i])):\n",
        "      if real_mask[i][j] > 0: real_mask[i][j] = 255\n",
        "      else: real_mask[i][j] = 0\n",
        "  \n",
        "  real_mask = torch.Tensor(real_mask)\n",
        "  mask_tensor = torch.Tensor(mask)\n",
        "\n",
        "  return real_mask, mask_tensor"
      ],
      "metadata": {
        "id": "rQiCA0G9Rt_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_iou_category(category, model, coefficient):\n",
        "  gt_list = []\n",
        "  pred_list = []\n",
        "  root = \"\"\n",
        "  dir_name = \"data/images/\" + category\n",
        "  for root, dirs, files in os.walk(os.path.join(root, dir_name)):\n",
        "    for file in files:\n",
        "      path_to_image = os.path.join(root, file)\n",
        "      \n",
        "      # If CAMERAS, then use computeAndSaveMaps() function, the rest CAM - get_saliency_map()\n",
        "      map_to_mask = get_saliency_map(model, path_to_image)\n",
        "      #map_to_mask = computeAndSaveMaps(model, path_to_image)\n",
        "      \n",
        "      gt_one, pred_one = get_masks(map_to_mask, path_to_image, coefficient=coefficient, resolution=resolution_model)\n",
        "      gt_list.append(gt_one)\n",
        "      pred_list.append(pred_one)\n",
        "  \n",
        "  gt_batch = torch.stack(gt_list)\n",
        "  pred_batch = torch.stack(pred_list)\n",
        "  iou = iou_pytorch(pred_batch, gt_batch).item()\n",
        "  return iou"
      ],
      "metadata": {
        "id": "fOvtlDT05PqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When coefficient > 120, regardless of the method, it returns a mask where the entire area is filled in.\n",
        "\n",
        "Select the coefficient using the for loop in the code below.\n",
        "\n",
        "cam_method_name - the variable responsible for the name of the method, **it must be changed**, since a dictionary with the values IoU and Coefficient is formed from this name.\n",
        "\n",
        "Change the CAM class and the layers that will be rendered in the function get_saliency_map(..), which is located at the beginning of the section \"Unsupervised segmentation pipeline\".\n",
        "\n",
        "Changes for CAMERAS need to be made to the function computeAndSaveMap(), which is located in the section \"CAMERAS\".\n",
        "\n",
        "Example from cam_method_name = 'mGradCAM-3.2and4.1'"
      ],
      "metadata": {
        "id": "Ng4lu4CZ5sR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cam_method_name = 'mGradCAM-3.2and4.1'\n",
        "\n",
        "category_name = 'drops'\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 7) # second parameter - number of classes\n",
        "torch.nn.init.xavier_normal_(model.fc.weight)\n",
        "model.load_state_dict(torch.load('ResNet18_Unfreeze3&4_4aug140epoch_model.pth')) # path to weights\n",
        "model.eval()\n",
        "model = model.cuda()\n",
        "\n",
        "category_classes = [category_name]\n",
        "category_best_iou = {}\n",
        "category_best_coefficient = {}\n",
        "\n",
        "for class_name in category_classes:\n",
        "  category_best_iou[class_name] = 0\n",
        "  category_best_coefficient[class_name] = 0\n",
        "\n",
        "for coefficient in range(1,100):\n",
        "  print(\"Coefficient: \", coefficient)\n",
        "  for class_name in category_classes:\n",
        "    value_category_iou = get_iou_category(class_name, model, coefficient=coefficient)\n",
        "\n",
        "    if category_best_iou.get(class_name) < value_category_iou:\n",
        "      category_best_iou[class_name] = value_category_iou\n",
        "      category_best_coefficient[class_name] = coefficient\n",
        "    \n",
        "# Saving the results\n",
        "with open(cam_method_name + '_best_iou', 'w') as f: \n",
        "  json.dump(category_best_iou, f)\n",
        "\n",
        "with open(cam_method_name + '_best_coefficient', 'w') as f: \n",
        "  json.dump(category_best_coefficient, f)"
      ],
      "metadata": {
        "id": "MtjFlJdGJM8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the following cell if you want to save to google drive"
      ],
      "metadata": {
        "id": "FlDz6KtUHtq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmd = 'cp ' + cam_method_name + '_best_iou' + ' gdrive/MyDrive/CaUS/CAM_IoU/'\n",
        "os.system(cmd)\n",
        "\n",
        "cmd = 'cp ' + cam_method_name + '_best_coefficient' + ' gdrive/MyDrive/CaUS/CAM_Coefficient/'\n",
        "os.system(cmd)"
      ],
      "metadata": {
        "id": "wJOIHzekHuCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating masks with the best coefficients"
      ],
      "metadata": {
        "id": "5iSbBcMNAB1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you received the results in the last session and want to get their masks, transfer the data from Google drive using the following cell"
      ],
      "metadata": {
        "id": "tRll6HTEGyxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R gdrive/MyDrive/CaUS/CAM_IoU/ /content/\n",
        "!cp -R gdrive/MyDrive/CaUS/CAM_Coefficient/ /content/"
      ],
      "metadata": {
        "id": "HBNFVBcNGyAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_masks(saliency_map, path_to_image, coefficient = 10, resolution = 224, path_to_dir = '', visualization=False):\n",
        "  if not isinstance(saliency_map, np.ndarray):\n",
        "    saliency_map = cv2.applyColorMap(np.uint8(255 * saliency_map.detach().numpy().squeeze()), cv2.COLORMAP_JET)\n",
        "  else:\n",
        "    saliency_map = cv2.applyColorMap(np.uint8(255 * saliency_map), cv2.COLORMAP_JET)\n",
        "  \n",
        "  hsv = cv2.cvtColor(saliency_map, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "  # Here we define the blue color range in HSV\n",
        "  lower_blue = np.array([coefficient,50,50])\n",
        "  upper_blue = np.array([130,255,255])\n",
        "\n",
        "  # This method creates a blue mask of the objects found in the frame\n",
        "  mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "  \n",
        "  # Inverting the mask\n",
        "  for i in range(len(mask)):\n",
        "    for j in range(len(mask[i])):\n",
        "      if mask[i][j] == 255: mask[i][j] = 0\n",
        "      else: mask[i][j] = 255\n",
        "\n",
        "  if visualization == True:\n",
        "    image = cv2.imread(path_to_image)\n",
        "    \n",
        "    img = image.copy()\n",
        "    img[mask == 255] = 0\n",
        "    color_channeled_image = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
        "    result = color_channeled_image * 0.4 + img * 0.7\n",
        "    mask_to_seg = saliency_map * 0.5 + image * 0.5\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
        "    ax1.set_title('Original picture')\n",
        "    ax2.set_title('The mask was created using CAM methods')\n",
        "    ax1.axis('off')\n",
        "    ax2.axis('off')\n",
        "    _ = ax1.imshow(cv2.cvtColor(mask_to_seg.astype(\"uint8\"), cv2.COLOR_BGR2RGB))\n",
        "    _ = ax2.imshow(cv2.cvtColor(result.astype(\"uint8\"), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  file_name = path_to_image.split('/')[-1]\n",
        "  # Second argument - gives Grey Scale Image\n",
        "  real_mask = cv2.imread(\"data/mask/\"+ path_to_image.split('/')[2] + \"/\" + file_name.split('.')[0] + \".png\", 0)\n",
        "  real_mask = cv2.resize(real_mask, (resolution, resolution), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "  for i in range(len(real_mask)):\n",
        "    for j in range(len(real_mask[i])):\n",
        "      if real_mask[i][j] > 0: real_mask[i][j] = 255\n",
        "      else: real_mask[i][j] = 0\n",
        "\n",
        "  path_for_class_dir = path_to_dir + '/' + file_name.split('_')[0]\n",
        "\n",
        "  try:\n",
        "    os.mkdir(path_for_class_dir)\n",
        "  except:\n",
        "    pass\n",
        "  \n",
        "  cv2.imwrite(path_for_class_dir + '/pred_' + file_name.split('.')[0] + '.png', mask)\n",
        "  cv2.imwrite(path_for_class_dir + '/gt_' + file_name.split('.')[0] + '.png', real_mask)\n",
        "  cv2.imwrite(path_for_class_dir + '/map_' + file_name.split('.')[0] + '.png', saliency_map)\n",
        "  \n",
        "  real_mask = torch.Tensor(real_mask)\n",
        "  mask_tensor = torch.Tensor(mask)\n",
        "\n",
        "  return real_mask, mask_tensor"
      ],
      "metadata": {
        "id": "wyZXZdtuwdhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_saliency_map(model, image_name, cam_method_name):\n",
        "  \n",
        "  if cam_method_name.split('-')[1] == '4.1and4.2':\n",
        "    target_layers = [model.layer4[-2], model.layer4[-1]]\n",
        "  elif cam_method_name.split('-')[1] == '4.2':\n",
        "    target_layers = [model.layer4[-1]]\n",
        "  elif cam_method_name.split('-')[1] == '3.2and4.1':\n",
        "    target_layers = [model.layer3[-1], model.layer4[-2]]\n",
        "  elif cam_method_name.split('-')[1] == '3.1and3.2and4.1and4.2':\n",
        "    target_layers = [model.layer3[-2], model.layer3[-1], model.layer4[-2], model.layer4[-1]]\n",
        "  elif cam_method_name.split('-')[1] == '3.2and4.2':\n",
        "    target_layers = [model.layer3[-1], model.layer4[-1]]\n",
        "  elif cam_method_name.split('-')[1] == '3.2and4.1and4.2':\n",
        "    target_layers = [model.layer3[-1], model.layer4[-2], model.layer4[-1]]\n",
        "  elif cam_method_name.split('-')[1] == '3.1and4.2':\n",
        "    target_layers = [model.layer3[-2], model.layer4[-1]]\n",
        "  elif cam_method_name.split('-')[1] == '3.1and4.1':\n",
        "    target_layers = [model.layer3[-2], model.layer4[-2]]\n",
        "\n",
        "\n",
        "  normalize = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225]\n",
        "  )\n",
        "\n",
        "  preprocess = transforms.Compose([\n",
        "    transforms.Resize((resolution_model,resolution_model)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "  ])\n",
        "\n",
        "  # load test image\n",
        "  img_pil = Image.open(image_name).convert('RGB')\n",
        "\n",
        "  img_tensor = preprocess(img_pil)\n",
        "  img_variable = Variable(img_tensor.unsqueeze(0))\n",
        "\n",
        "  input_tensor = img_variable.cuda()\n",
        "\n",
        "  # This specifies which CAM method will be used\n",
        "  if cam_method_name.split('-')[0] == 'GradCAM' or cam_method_name.split('-')[0] == 'mGradCAM':\n",
        "    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
        "  elif cam_method_name.split('-')[0] == 'LayerCAM':\n",
        "    cam = LayerCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
        "\n",
        "  grayscale_cam = cam(input_tensor=input_tensor)\n",
        "  # In this example grayscale_cam has only one image in the batch:\n",
        "  grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "  return grayscale_cam"
      ],
      "metadata": {
        "id": "B3_gw-Lx3n8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_iou_category(category, model, coefficient, cam_method_name, path_to_dir):\n",
        "  root = \"\"\n",
        "  dir_name = \"data/images/\" + category\n",
        "  for root, dirs, files in os.walk(os.path.join(root, dir_name)):\n",
        "    for file in files:\n",
        "      path_to_image = os.path.join(root, file)\n",
        "\n",
        "      # If CAMERAS, then use computeAndSaveMaps() function, the rest CAM - get_saliency_map()\n",
        "      #map_to_mask = get_saliency_map(model, path_to_image)\n",
        "      map_to_mask = computeAndSaveMaps(model, path_to_image)\n",
        "      \n",
        "      gt_one, pred_one = get_masks(map_to_mask, path_to_image, coefficient=coefficient, resolution=resolution_model, path_to_dir=path_to_dir)"
      ],
      "metadata": {
        "id": "EqQFJoSf3n8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "absolute_path_dir = \"CAM_Coefficient/\"\n",
        "absolute_dir_names = os.listdir(absolute_path_dir)\n",
        "\n",
        "print(absolute_dir_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJdlbckPydgy",
        "outputId": "b530247a-535c-4e9f-b52b-cde2818f1d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['GradCAM-4.1and4.2_best_coefficient', 'LayerCAM-3.1and4.1_best_coefficient', 'LayerCAM-4.1and4.2_best_coefficient', 'mGradCAM-4.1and4.2_best_coefficient', 'mGradCAM-3.1and3.2and4.1and4.2_best_coefficient', 'mGradCAM-3.1and4.1_best_coefficient', 'GradCAM-3.2and4.2_best_coefficient', 'LayerCAM-3.1and3.2and4.1and4.2_best_coefficient', 'mGradCAM-3.2and4.1_best_coefficient', 'LayerCAM-3.2and4.1and4.2_best_coefficient', 'CAMERAS-custom-res_best_coefficient', 'LayerCAM-3.2and4.1_best_coefficient', 'LayerCAM-4.2_best_coefficient', 'mGradCAM-3.1and4.2_best_coefficient', 'LayerCAM-3.1and4.2_best_coefficient', 'CAMERAS-classic_best_coefficient', 'LayerCAM-3.2and4.2_best_coefficient', 'GradCAM-4.2_best_coefficient', 'mGradCAM-3.2and4.2_best_coefficient']\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GradCAM and LayerCAM\n",
        "# name_dict_list = ['GradCAM-4.1and4.2_best_coefficient', 'LayerCAM-4.1and4.2_best_coefficient', 'GradCAM-4.2_best_coefficient', 'LayerCAM-3.2and4.1_best_coefficient', 'LayerCAM-3.1and3.2and4.1and4.2_best_coefficient', 'LayerCAM-4.2_best_coefficient', 'GradCAM-3.2and4.2_best_coefficient', 'LayerCAM-3.2and4.1and4.2_best_coefficient', 'LayerCAM-3.2and4.2_best_coefficient', 'LayerCAM-3.1and4.2_best_coefficient', 'LayerCAM-3.1and4.1_best_coefficient']\n",
        "\n",
        "# mGradCAM\n",
        "# name_dict_list = ['mGradCAM-4.1and4.2_best_coefficient', 'mGradCAM-3.1and3.2and4.1and4.2_best_coefficient', 'mGradCAM-3.1and4.1_best_coefficient', 'mGradCAM-3.2and4.1_best_coefficient', 'mGradCAM-3.1and4.2_best_coefficient', 'mGradCAM-3.2and4.2_best_coefficient']\n",
        "\n",
        "# CAMERAS classic\n",
        "# name_dict_list = ['CAMERAS-classic_best_coefficient']\n",
        "\n",
        "# CAMERAS custom\n",
        "name_dict_list = ['CAMERAS-custom-res_best_coefficient']"
      ],
      "metadata": {
        "id": "d82sgcb-BTKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir paper"
      ],
      "metadata": {
        "id": "BryOACKbh99x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "model = resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 7) # second parameter - number of classes\n",
        "torch.nn.init.xavier_normal_(model.fc.weight)\n",
        "model.load_state_dict(torch.load('ResNet18_Unfreeze3&4_4aug140epoch_model.pth')) # path to weights\n",
        "model.eval()\n",
        "model = model.cuda()\n",
        "\n",
        "category_name = 'drops'\n",
        "\n",
        "cam_method_list = []\n",
        "category_classes = [category_name]\n",
        "\n",
        "for name_dict in name_dict_list:\n",
        "  cam_method_list.append(name_dict.split('_')[0])\n",
        "\n",
        "for cam_method_name in cam_method_list:\n",
        "  path_to_cam_method_name = 'paper/' + cam_method_name\n",
        "\n",
        "  with open('CAM_Coefficient/' + cam_method_name + '_best_coefficient', 'r') as f:\n",
        "    category_best_coefficient = json.load(f)\n",
        "  \n",
        "  try:\n",
        "    os.mkdir(path_to_cam_method_name)\n",
        "  except:\n",
        "    pass\n",
        "  \n",
        "  for class_name in category_classes:\n",
        "    coefficient = category_best_coefficient.get(class_name)\n",
        "    value_category_iou = get_iou_category(class_name, model, coefficient=coefficient,cam_method_name = cam_method_name,path_to_dir=path_to_cam_method_name)"
      ],
      "metadata": {
        "id": "xEj9dFUCxJ4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r paper.zip paper/"
      ],
      "metadata": {
        "id": "W4x8DEiHEvZy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}